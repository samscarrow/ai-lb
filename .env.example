# Example Environment Variables

# Redis Configuration
REDIS_PORT=6379

# Monitor Configuration
# A comma-separated list of hostnames or IPs to check for LLM servers.
# Use host.docker.internal to refer to the machine running Docker.
SCAN_HOSTS=host.docker.internal
SCAN_PORTS=9999
SCAN_INTERVAL=60
# Optional concurrency caps for LEAST_LOADED routing
DEFAULT_MAXCONN=0
MAXCONN_MAP=

# Load Balancer Configuration
LOAD_BALANCER_PORT=8000

# Routing and attempts
ROUTING_STRATEGY=P2C
MIN_HEALTHY_NODES=1
REQUEST_TIMEOUT_SECS=60
ATTEMPTS_PER_MODEL=3
RETRY_BACKOFF_MS=50,100

# Circuit breaker
CIRCUIT_BREAKER_THRESHOLD=3
CIRCUIT_BREAKER_COOLDOWN_SECS=60

# Auto selection
MODEL_SENTINELS=auto,default
# Preference order for auto model selection (comma/semicolon separated)
PREFERRED_MODELS=
AUTO_MIN_NODES=2
STRICT_AUTO_MODE=0
LM_PREFER_SMALL=1
CROSS_MODEL_FALLBACK=0

# Stickiness (only applies when x-session-id header is present)
STICKY_SESSIONS_ENABLED=false
STICKY_TTL_SECS=600

# P2C tuning
P2C_ALPHA=0.5
P2C_PENALTY_WEIGHT=2.0

# Hedging (metric scaffolding present; request duplication WIP)
HEDGING_ENABLED=true
HEDGING_SMALL_MODELS_ONLY=true
HEDGING_MAX_DELAY_MS=800
HEDGING_P95_FRACTION=0.6

# Eligibility thresholds
MAX_P95_LATENCY_SECS=5.0

# Cloud backend configuration
# Register cloud API providers to enable unified routing across local and cloud backends.
# Cloud backends are registered without health probing (cloud APIs don't expose /v1/models).
# Format: "name=url|api_key[|provider_type]" (comma or semicolon separated)
# provider_type is optional, defaults to "openai". Use "anthropic" for Anthropic Claude API.
# URL should include /v1 path for OpenAI-compatible endpoints.
# Example: CLOUD_BACKENDS=openai=https://api.openai.com/v1|sk-xxx|openai,claude=https://api.anthropic.com/v1|sk-ant-xxx|anthropic
CLOUD_BACKENDS=

# Cloud models configuration - which models to advertise for each cloud backend
# Format: "backend_name=model1,model2,model3;backend_name2=modelA,modelB"
# Example: CLOUD_MODELS=openai=gpt-4o,gpt-4o-mini,gpt-3.5-turbo;claude=claude-sonnet-4-20250514,claude-3-haiku
CLOUD_MODELS=

# Rate limiting backoff for cloud providers
# When a cloud provider returns HTTP 429, ai-lb will back off with exponential delay.
RATE_LIMIT_BACKOFF_BASE_SECS=1.0
RATE_LIMIT_BACKOFF_MAX_SECS=60.0
RATE_LIMIT_BACKOFF_JITTER=0.3

# Fallback chain configuration
# Define named fallback chains for sequential failover across backends.
# Format: "chain_name=backend1>backend2>backend3;chain2=..."
# Per-backend options: backend(timeout=30,retries=2)
# Special backend "local:auto" selects best available local backend.
# Example: FALLBACK_CHAINS=reliable=cloud:openai(timeout=30,retries=2)>cloud:claude(timeout=45)>local:auto(timeout=60)
FALLBACK_CHAINS=

# Default fallback chain name (if set, will be used when no chain specified in request)
DEFAULT_FALLBACK_CHAIN=

# Total timeout for fallback chain execution (all backends combined)
FALLBACK_TOTAL_TIMEOUT_SECS=120
