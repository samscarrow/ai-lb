# Example Environment Variables

# Redis Configuration
REDIS_PORT=6379

# Monitor Configuration
# A comma-separated list of hostnames or IPs to check for LLM servers.
# Use host.docker.internal to refer to the machine running Docker.
SCAN_HOSTS=host.docker.internal
SCAN_PORTS=9999
SCAN_INTERVAL=60
# Optional concurrency caps for LEAST_LOADED routing
DEFAULT_MAXCONN=0
MAXCONN_MAP=

# Load Balancer Configuration
LOAD_BALANCER_PORT=8000

# Routing and attempts
ROUTING_STRATEGY=P2C
MIN_HEALTHY_NODES=1
REQUEST_TIMEOUT_SECS=60
ATTEMPTS_PER_MODEL=3
RETRY_BACKOFF_MS=50,100

# Circuit breaker
CIRCUIT_BREAKER_THRESHOLD=3
CIRCUIT_BREAKER_COOLDOWN_SECS=60

# Auto selection
MODEL_SENTINELS=auto,default
# Preference order for auto model selection (comma/semicolon separated)
PREFERRED_MODELS=
AUTO_MIN_NODES=2
STRICT_AUTO_MODE=0
LM_PREFER_SMALL=1
CROSS_MODEL_FALLBACK=0

# Stickiness (only applies when x-session-id header is present)
STICKY_SESSIONS_ENABLED=false
STICKY_TTL_SECS=600

# P2C tuning
P2C_ALPHA=0.5
P2C_PENALTY_WEIGHT=2.0

# Hedging (metric scaffolding present; request duplication WIP)
HEDGING_ENABLED=true
HEDGING_SMALL_MODELS_ONLY=true
HEDGING_MAX_DELAY_MS=800
HEDGING_P95_FRACTION=0.6

# Eligibility thresholds
MAX_P95_LATENCY_SECS=5.0

# Cloud backend configuration
# Register cloud API providers to enable unified routing across local and cloud backends.
# Cloud backends are registered without health probing (cloud APIs don't expose /v1/models).
# Format: "name=url|api_key,name2=url2|api_key2" (comma or semicolon separated)
# URL should include /v1 path for OpenAI-compatible endpoints.
# Example: CLOUD_BACKENDS=openai=https://api.openai.com/v1|sk-xxx,anthropic=https://api.anthropic.com/v1|sk-ant-xxx
CLOUD_BACKENDS=

# Cloud models configuration - which models to advertise for each cloud backend
# Format: "backend_name=model1,model2,model3;backend_name2=modelA,modelB"
# Example: CLOUD_MODELS=openai=gpt-4o,gpt-4o-mini,gpt-3.5-turbo;anthropic=claude-sonnet-4-20250514
CLOUD_MODELS=

# Rate limiting backoff for cloud providers
# When a cloud provider returns HTTP 429, ai-lb will back off with exponential delay.
RATE_LIMIT_BACKOFF_BASE_SECS=1.0
RATE_LIMIT_BACKOFF_MAX_SECS=60.0
RATE_LIMIT_BACKOFF_JITTER=0.3
