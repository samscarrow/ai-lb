import httpx
import random
import redis.asyncio as redis
import json
import logging
import time
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple
from fastapi import FastAPI, Request, Response, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
import uuid
import asyncio
from contextlib import asynccontextmanager

from . import config
from .request_validation import sanitize_chat_request, sanitize_embeddings_request
from .routing.strategies import get_routing_strategy
from .execution import (
    ExecutionMode,
    ExecutionConfig,
    BackendResult,
    ConsensusResult,
    ExecutionEngine,
)
from .providers import get_adapter

@asynccontextmanager
async def lifespan(app: FastAPI):
    global redis_client, http_client, router
    logger.info("Load balancer starting up...")
    if redis_client is None:
        # Create Redis client with reasonable timeouts
        redis_client = redis.Redis(
            host=config.REDIS_HOST,
            port=config.REDIS_PORT,
            decode_responses=True,
            socket_connect_timeout=2,  # 2 second connection timeout
            socket_timeout=5,  # 5 second socket timeout
            retry_on_timeout=True,
        )
        # Try quick connection test, but don't block startup if it fails
        try:
            await asyncio.wait_for(redis_client.ping(), timeout=2.0)
            logger.info("Successfully connected to Redis")
        except Exception as e:
            logger.warning(f"Redis not immediately available: {e}. Will retry in background...")
    if http_client is None:
        http_client = httpx.AsyncClient(timeout=httpx.Timeout(config.REQUEST_TIMEOUT_SECS))
    if router is None:
        try:
            router = get_routing_strategy(config.ROUTING_STRATEGY)
            logger.info("Using routing strategy: %s", config.ROUTING_STRATEGY)
        except ValueError as e:
            logger.error("Error selecting strategy: %s", e)
            router = get_routing_strategy("ROUND_ROBIN")
            logger.info("Defaulting to routing strategy: ROUND_ROBIN")
    try:
        yield
    finally:
        if redis_client is not None:
            try:
                await redis_client.close()
            except Exception:
                pass
        if http_client is not None:
            try:
                await http_client.aclose()
            except Exception:
                pass
        logger.info("Load balancer shut down.")


app = FastAPI(title="AI Load Balancer", lifespan=lifespan)
redis_client = None
http_client = None
router = None
logger = logging.getLogger("ai_lb")
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

# Helper to resolve circuit breaker cooldown consistently across legacy and new config
def _cb_cooldown_secs() -> int:
    try:
        return int(getattr(config, "CIRCUIT_BREAKER_COOLDOWN_SECS", config.CIRCUIT_BREAKER_TTL_SECS))
    except Exception:
        return 60

async def _series_p95(model: str, node: str) -> float:
    """Approximate p95 from cumulative histogram buckets stored in Redis.
    Returns 0.0 if no data.
    """
    try:
        buckets = _LAT_BUCKETS
        series_key = f"{model}|{node}"
        cumulative_counts = []
        total_count = 0
        for le in buckets:
            val = await redis_client.get(f"lb:latency_bucket:{series_key}:{le}")
            count = int(val) if val else 0
            cumulative_counts.append(count)
            if le == float("inf"):
                total_count = count
        if total_count == 0:
            return 0.0
        p95_target = total_count * 0.95
        for i, count in enumerate(cumulative_counts):
            if count >= p95_target:
                if i == 0:
                    return buckets[i] * 0.5
                lower = buckets[i-1]
                upper = buckets[i]
                prev = cumulative_counts[i-1]
                rng = max(1, count - prev)
                pos = (p95_target - prev) / rng
                if upper == float("inf"):
                    return lower * 1.1  # Slight bump if in inf bucket
                return lower + pos * (upper - lower)
        return 0.0
    except Exception:
        return 0.0

async def _series_count(model: str, node: str) -> int:
    try:
        series_key = f"{model}|{node}"
        val = await redis_client.get(f"lb:latency_count:{series_key}")
        return int(val) if val else 0
    except Exception:
        return 0

async def _estimate_pool_p95(model: str, nodes: list[str]) -> float:
    """Estimate p95 latency for a model across a pool using median of node p95s
    that have sufficient samples; fallback to 1.0s if unknown."""
    try:
        vals = []
        for n in nodes:
            cnt = await _series_count(model, n)
            if cnt >= getattr(config, "ELIGIBILITY_MIN_P95_SAMPLES", 20):
                p = await _series_p95(model, n)
                if p:
                    vals.append(p)
        if not vals:
            return 1.0
        vals.sort()
        m = len(vals) // 2
        return vals[m] if len(vals) % 2 else (vals[m-1] + vals[m]) / 2.0
    except Exception:
        return 1.0

def _is_small_model(model: str) -> bool:
    try:
        classes = getattr(config, "MODEL_CLASSES", {}) or {}
        small = classes.get("historical_small", {})
        return model in set(small.get("candidates", []))
    except Exception:
        return False

async def get_eligible_nodes(model_name: str, include_cloud: bool = True):
    """Find healthy nodes that advertise the model and meet eligibility: not tripped, not rate-limited, and under p95 threshold.

    When include_cloud=True, also includes cloud backends that support the model.
    """
    healthy_nodes = sorted(await redis_client.smembers("nodes:healthy"))
    eligible_nodes = []
    for node in healthy_nodes:
        # Skip nodes on open circuit for this period
        if await _is_circuit_open(node):
            continue
        # Skip nodes that are currently rate-limited (429 backoff)
        if await _is_rate_limited(node):
            continue
        models_json = await redis_client.get(f"node:{node}:models")
        if not models_json:
            continue
        try:
            models = json.loads(models_json).get("data", [])
        except Exception:
            models = []
        # Either the node advertises the model in /v1/models OR monitor warmed it and marked supports:{model}
        supports_key = f"node:{node}:supports:{model_name}"
        supports = await redis_client.get(supports_key)
        if any(m.get("id") == model_name for m in models) or (supports is not None and supports != "0"):
            # Enforce p95 threshold if configured (>0)
            try:
                max_p95 = float(getattr(config, "MAX_P95_LATENCY_SECS", 0) or 0)
            except Exception:
                max_p95 = 0.0
            if max_p95 > 0:
                # Require a minimum number of samples before enforcing p95 threshold
                cnt = await _series_count(model_name, node)
                if cnt >= getattr(config, "ELIGIBILITY_MIN_P95_SAMPLES", 20):
                    p95 = await _series_p95(model_name, node)
                    if p95 and p95 > max_p95:
                        # Defer: skip node until it recovers
                        continue
            eligible_nodes.append(node)

    # Add eligible cloud backends
    if include_cloud:
        cloud_nodes = await _get_eligible_cloud_backends(model_name)
        eligible_nodes.extend(cloud_nodes)

    return eligible_nodes

async def _aggregate_models() -> list:
    """Return a list of unique model dicts aggregated across healthy nodes."""
    healthy_nodes = sorted(await redis_client.smembers("nodes:healthy"))
    all_models = {}
    for node in healthy_nodes:
        models_json = await redis_client.get(f"node:{node}:models")
        if models_json:
            try:
                models = json.loads(models_json).get("data", [])
            except Exception:
                models = []
            for model in models:
                mid = model.get("id")
                if mid and mid not in all_models:
                    all_models[mid] = model
    return list(all_models.values())

async def _aggregate_models_by_node() -> dict:
    """Return mapping node -> list of model ids for healthy nodes."""
    healthy_nodes = sorted(await redis_client.smembers("nodes:healthy"))
    out = {}
    for node in healthy_nodes:
        models_json = await redis_client.get(f"node:{node}:models")
        ids = []
        if models_json:
            try:
                models = json.loads(models_json).get("data", [])
                ids = [m.get("id") for m in models if m.get("id")]
            except Exception:
                ids = []
        out[node] = ids
    return out

def _is_model_sentinel(name: Optional[str]) -> bool:
    if not name:
        return False
    low = str(name).strip().lower()
    return low in config.MODEL_SENTINELS or low == config.LM_MODEL_SENTINEL

async def _resolve_auto_model(prefer_intersection: bool = False) -> Optional[str]:
    """Choose a model for auto/default using class fallbacks and min_nodes.
    Order classes by LM_PREFER_SMALL; within each class, pick the first candidate with
    ≥ AUTO_MIN_NODES eligible nodes. If none and STRICT_AUTO_MODE is false, allow ≥1.
    Falls back to previous union/intersection strategy if needed.
    """
    by_node = await _aggregate_models_by_node()
    if not by_node:
        return None

    async def _eligible_count(mid: str) -> int:
        nodes = await get_eligible_nodes(mid, include_cloud=False)
        return len(nodes)

    classes_cfg = getattr(config, "MODEL_CLASSES", {}) or {}
    if classes_cfg:
        # Determine class order
        prefer_small = bool(getattr(config, "LB_PREFER_SMALL", False))
        ordered = []
        if prefer_small:
            for name in ("historical_small", "historical_medium"):
                if name in classes_cfg:
                    ordered.append(name)
        else:
            for name in ("historical_medium", "historical_small"):
                if name in classes_cfg:
                    ordered.append(name)
        # Add any remaining classes not explicitly ordered
        for cname in classes_cfg.keys():
            if cname not in ordered:
                ordered.append(cname)

        # Try classes in order
        for cname in ordered:
            c = classes_cfg.get(cname) or {}
            candidates = c.get("candidates", [])
            min_nodes = int(c.get("min_nodes") or getattr(config, "AUTO_MIN_NODES", 2))
            # Strict pass
            for mid in candidates:
                if (await _eligible_count(mid)) >= min_nodes:
                    return mid
            # Soft pass if not strict
            if not getattr(config, "STRICT_AUTO_MODE", False):
                for mid in candidates:
                    if (await _eligible_count(mid)) >= 1:
                        return mid

    # Back-compat: previous behavior (preferred models → weights → union/intersection)
    ids: List[str] = []
    if prefer_intersection or config.AUTO_MODEL_STRATEGY.lower() == "intersection_first":
        sets = [set(v) for v in by_node.values() if v]
        inter = set.intersection(*sets) if sets else set()
        ids = list(inter) if inter else []
    if not ids:
        seen = set()
        for v in by_node.values():
            for mid in v:
                if mid not in seen:
                    seen.add(mid)
                    ids.append(mid)
    # Filter union by availability (≥1 eligible node) to avoid mapping to dead models
    filtered: List[str] = []
    for mid in ids:
        try:
            if (await get_eligible_nodes(mid)):
                filtered.append(mid)
        except Exception:
            continue
    ids = filtered
    # Prefer configured priorities
    for want in getattr(config, "PREFERRED_MODELS", []):
        if want in ids:
            return want
    try:
        weights = dict(getattr(config, "MODEL_WEIGHTS", {}))
        wjson = await redis_client.get("lb:model_weights")
        if wjson:
            weights.update(json.loads(wjson))
        if weights:
            best_id = None
            best_w = None
            for mid in ids:
                w = weights.get(mid)
                if w is not None and (best_w is None or w > best_w):
                    best_w = w
                    best_id = mid
            if best_id is not None:
                return best_id
    except Exception:
        pass
    return ids[0] if ids else None

@app.get("/v1/nodes")
async def list_nodes():
    healthy_nodes = await redis_client.smembers("nodes:healthy")
    out = []
    for n in healthy_nodes:
        inflight = await redis_client.get(f"node:{n}:inflight")
        failures = await redis_client.get(f"node:{n}:failures")
        maxconn = await redis_client.get(f"node:{n}:maxconn")
        out.append({
            "node": n,
            "inflight": int(inflight or 0),
            "failures": int(failures or 0),
            "maxconn": int(maxconn or 0),
        })
    return {"data": out}

@app.get("/v1/eligible_nodes")
async def list_eligible_nodes(model: str):
    return {"data": await get_eligible_nodes(model)}

@app.get("/v1/debug/eligible")
async def debug_eligible(model: str):
    """Debug endpoint: shows how eligibility is computed per node."""
    healthy_nodes = sorted(await redis_client.smembers("nodes:healthy"))
    details = []
    for node in healthy_nodes:
        item = {"node": node, "has_model": False, "cb_open": False, "p95": None, "skipped": False, "reason": ""}
        try:
            models_json = await redis_client.get(f"node:{node}:models")
            models = json.loads(models_json).get("data", []) if models_json else []
            has = any(m.get("id") == model for m in models)
            item["has_model"] = has
            item["cb_open"] = await _is_circuit_open(node)
            p95 = await _series_p95(model, node)
            item["p95"] = p95
            # Apply same logic as get_eligible_nodes
            if not has:
                item["skipped"] = True
                item["reason"] = "model_missing"
            elif item["cb_open"]:
                item["skipped"] = True
                item["reason"] = "circuit_open"
            else:
                max_p95 = float(getattr(config, "MAX_P95_LATENCY_SECS", 0) or 0)
                if max_p95 > 0 and p95 and p95 > max_p95:
                    item["skipped"] = True
                    item["reason"] = f"p95>{max_p95}"
        except Exception as e:
            item["skipped"] = True
            item["reason"] = f"error:{e}"
        details.append(item)
    eligible = [d["node"] for d in details if not d["skipped"] and d["has_model"] and not d["cb_open"]]
    return {"healthy": healthy_nodes, "eligible": eligible, "details": details}

async def _inc_inflight(node: str):
    try:
        await redis_client.incrby(f"node:{node}:inflight", 1)
    except Exception:
        pass

async def _dec_inflight(node: str):
    try:
        await redis_client.incrby(f"node:{node}:inflight", -1)
    except Exception:
        pass

class CapacityError(Exception):
    def __init__(self, scope: str = "node"):
        self.scope = scope
        super().__init__(f"{scope} at capacity")


class RateLimitError(Exception):
    """Raised when a backend returns HTTP 429 rate limit response."""
    def __init__(self, backend: str, retry_after: Optional[float] = None):
        self.backend = backend
        self.retry_after = retry_after
        super().__init__(f"Rate limited by {backend}")

async def _acquire_slot(node: str) -> bool:
    """Atomically increment inflight and enforce optional maxconn.
    If maxconn is set and would be exceeded, revert and return False.
    """
    key = f"node:{node}:inflight"
    try:
        new_val = await redis_client.incrby(key, 1)
        max_val = await redis_client.get(f"node:{node}:maxconn")
        if max_val not in (None, "", "0"):
            try:
                m = int(max_val)
            except Exception:
                m = None
            if m is not None and int(new_val) > m:
                # revert and signal capacity reached
                await redis_client.incrby(key, -1)
                return False
        return True
    except Exception:
        # On any Redis issue, allow (fail-open) to avoid total outage
        return True

async def _acquire_model_slot(model: str) -> bool:
    key = f"model:{model}:inflight"
    try:
        new_val = await redis_client.incrby(key, 1)
        max_val = await redis_client.get(f"model:{model}:maxconn")
        if max_val not in (None, "", "0"):
            try:
                m = int(max_val)
            except Exception:
                m = None
            if m is not None and int(new_val) > m:
                await redis_client.incrby(key, -1)
                return False
        return True
    except Exception:
        return True

async def _dec_model(model: str):
    try:
        await redis_client.incrby(f"model:{model}:inflight", -1)
    except Exception:
        pass

async def _penalize_failure(node: str):
    try:
        key = f"node:{node}:failures"
        await redis_client.incrby(key, 1)
        await redis_client.expire(key, config.FAILURE_PENALTY_TTL_SECS)
    except Exception:
        pass

async def _record_failure(node: str):
    try:
        key = f"node:{node}:failures"
        failures = await redis_client.incrby(key, 1)
        await redis_client.expire(key, _cb_cooldown_secs())
        if failures >= config.CIRCUIT_BREAKER_THRESHOLD:
            await redis_client.set(f"node:{node}:cb_open", "1")
            await redis_client.expire(f"node:{node}:cb_open", _cb_cooldown_secs())
    except Exception:
        pass

async def _record_success(node: str):
    try:
        await redis_client.set(f"node:{node}:failures", 0)
        await redis_client.expire(f"node:{node}:failures", _cb_cooldown_secs())
        # closing circuit simply by letting cb_open expire; do nothing here
    except Exception:
        pass

async def _is_circuit_open(node: str) -> bool:
    try:
        val = await redis_client.get(f"node:{node}:cb_open")
        return val is not None and val != "0"
    except Exception:
        return False


# ---------------------- Rate Limit Backoff Tracking ----------------------


async def _record_rate_limit(node: str, retry_after_secs: Optional[float] = None):
    """Record a rate limit (429) response from a backend.

    Marks the backend as temporarily unhealthy with exponential backoff.
    Uses Retry-After header value if available, otherwise exponential backoff.
    """
    try:
        # Increment rate limit counter for this node
        count_key = f"node:{node}:rate_limit_count"
        count = await redis_client.incrby(count_key, 1)
        await redis_client.expire(count_key, 300)  # Reset after 5 minutes

        # Calculate backoff duration
        if retry_after_secs and retry_after_secs > 0:
            # Use Retry-After from response
            backoff_secs = min(retry_after_secs, config.RATE_LIMIT_BACKOFF_MAX_SECS)
        else:
            # Exponential backoff: base * 2^(count-1)
            base = config.RATE_LIMIT_BACKOFF_BASE_SECS
            max_backoff = config.RATE_LIMIT_BACKOFF_MAX_SECS
            backoff_secs = min(base * (2 ** (count - 1)), max_backoff)

        # Add jitter (±jitter%)
        jitter_range = backoff_secs * config.RATE_LIMIT_BACKOFF_JITTER
        jitter = random.uniform(-jitter_range, jitter_range)
        backoff_secs = max(0.1, backoff_secs + jitter)

        # Mark backend as rate-limited (temporary unhealthy state)
        limit_key = f"node:{node}:rate_limited"
        await redis_client.set(limit_key, "1")
        await redis_client.expire(limit_key, int(backoff_secs) + 1)

        # Track metrics
        await redis_client.incrby("lb:rate_limits_total", 1)
        await redis_client.incrby(f"lb:rate_limits:{node}", 1)

        logger.warning(
            "[rate_limit] Backend %s rate limited, backing off for %.1f seconds (count: %d)",
            node, backoff_secs, count
        )
    except Exception as e:
        logger.warning("[rate_limit] Failed to record rate limit for %s: %s", node, e)


async def _is_rate_limited(node: str) -> bool:
    """Check if a backend is currently rate-limited and in backoff."""
    try:
        val = await redis_client.get(f"node:{node}:rate_limited")
        return val is not None and val != "0"
    except Exception:
        return False


async def _clear_rate_limit(node: str):
    """Clear rate limit state for a backend after successful request."""
    try:
        # Clear the rate-limited flag
        await redis_client.delete(f"node:{node}:rate_limited")
        # Reset the count (on success, we reset exponential backoff)
        await redis_client.delete(f"node:{node}:rate_limit_count")
    except Exception:
        pass


async def _inc_requests_total():
    try:
        await redis_client.incrby("lb:requests_total", 1)
    except Exception:
        pass

async def get_eligible_nodes_for_model(model_name: str):
    return await get_eligible_nodes(model_name)

async def _find_fallback_model(current: str) -> Optional[str]:
    """Pick the next candidate model from the same class list that meets min_nodes.
    Returns None if no suitable fallback is found or if fallback disabled.
    """
    if not getattr(config, "CROSS_MODEL_FALLBACK", False):
        return None
    classes = getattr(config, "MODEL_CLASSES", {}) or {}
    for cname, cfg in classes.items():
        cands = cfg.get("candidates", [])
        if current in cands:
            min_nodes = int(cfg.get("min_nodes") or getattr(config, "AUTO_MIN_NODES", 2))
            for mid in cands:
                if mid == current:
                    continue
                nodes = await get_eligible_nodes(mid, include_cloud=False)
                if len(nodes) >= min_nodes:
                    return mid
    return None

async def _warm_model_on_nodes(model: str, nodes: list[str]) -> None:
    """Best-effort tiny POST to prompt backend to load the model on selected nodes."""
    if not nodes:
        return
    payload = {"model": model, "messages": [{"role": "user", "content": "ping"}], "max_tokens": 4, "stream": False}
    timeout = httpx.Timeout(float(getattr(config, "ON_DEMAND_WARM_TIMEOUT_SECS", 30)))
    async def one(node: str):
        url = f"http://{node}/v1/chat/completions"
        try:
            r = await http_client.post(url, json=payload, timeout=timeout)
            # Mark support key to speed up eligibility on success
            if r.status_code == 200:
                try:
                    await redis_client.set(f"node:{node}:supports:{model}", 1)
                    await redis_client.expire(f"node:{node}:supports:{model}", 300)
                except Exception:
                    pass
        except Exception:
            return
    # Launch concurrently
    await asyncio.gather(*[one(n) for n in nodes])

async def _on_demand_wait_for_model(model: str) -> list[str]:
    """If no nodes are eligible, probe a few healthy nodes to load the model and poll until any become eligible or grace window elapses."""
    if not getattr(config, "ON_DEMAND_WAIT_ENABLED", True):
        return []
    try:
        healthy = list(await redis_client.smembers("nodes:healthy"))
    except Exception:
        healthy = []
    if not healthy:
        return []
    # Choose up to N nodes to probe
    fanout = max(1, int(getattr(config, "ON_DEMAND_WARM_FANOUT", 2)))
    targets = healthy if len(healthy) <= fanout else random.sample(healthy, fanout)
    await _warm_model_on_nodes(model, targets)

    # Poll for eligibility up to grace window
    grace = float(getattr(config, "ON_DEMAND_WARM_GRACE_SECS", 30))
    poll_ms = max(100, int(getattr(config, "ON_DEMAND_WARM_POLL_MS", 750)))
    deadline = time.monotonic() + grace
    while time.monotonic() < deadline:
        nodes = await get_eligible_nodes(model)
        if nodes:
            return nodes
        await asyncio.sleep(poll_ms / 1000.0)
    return []

async def _get_sticky_node(session_id: Optional[str], model_name: str) -> Optional[str]:
    # Stickiness requires feature flag enabled and a provided session id
    if not getattr(config, "STICKY_SESSIONS_ENABLED", False) or not session_id:
        return None
    try:
        return await redis_client.get(f"session:{session_id}:{model_name}")
    except Exception:
        return None

async def _set_sticky_node(session_id: Optional[str], model_name: str, node: str):
    # Stickiness requires feature flag enabled and a provided session id
    if not getattr(config, "STICKY_SESSIONS_ENABLED", False) or not session_id:
        return
    try:
        key = f"session:{session_id}:{model_name}"
        await redis_client.set(key, node)
        await redis_client.expire(key, config.STICKY_TTL_SECS)
    except Exception:
        pass


# ---------------------- Multi-Backend Execution Helpers ----------------------


@dataclass
class ExtendedExecutionConfig:
    """Extended execution config with oracle and fallback chain support."""
    base: ExecutionConfig
    # Oracle configuration for consensus mode
    oracle_backend: Optional[str] = None  # Validated oracle backend (e.g., "cloud:openai")
    oracle_valid: bool = True  # Whether the oracle header was valid
    # Fallback chain configuration
    fallback_chain: Optional[str] = None  # Name of fallback chain to use


def _parse_execution_mode(request: Request) -> Optional[ExtendedExecutionConfig]:
    """Parse execution mode from request headers or query params.

    Headers (preferred):
        X-Execution-Mode: race | all | sequence | consensus
        X-Target-Backends: alias1,alias2,... (optional)
        X-Max-Backends: N (optional, default 3)
        X-Consensus-Oracle: openai | cloud:openai (optional, for consensus mode)
        X-Fallback-Chain: chain_name (optional, takes precedence over X-Execution-Mode)

    Query params (fallback):
        ?mode=race|all|sequence|consensus
        &backends=alias1,alias2,...
        &max_backends=N

    Precedence rules:
    - X-Fallback-Chain takes precedence over X-Execution-Mode
    - If both are set, X-Execution-Mode is ignored (logged as warning)
    """
    if not getattr(config, "MULTI_EXEC_ENABLED", True):
        return None

    # Check for fallback chain first (takes precedence)
    fallback_chain = request.headers.get("x-fallback-chain")
    if fallback_chain:
        fallback_chain = fallback_chain.strip()
        chains = getattr(config, "FALLBACK_CHAINS", {}) or {}
        if fallback_chain not in chains:
            # Invalid chain name - return None to use standard routing
            logger.warning(f"Unknown fallback chain: {fallback_chain}")
            return None
        # Log if execution mode was also set
        if request.headers.get("x-execution-mode"):
            logger.warning(
                f"Both X-Fallback-Chain ({fallback_chain}) and X-Execution-Mode set. "
                "Fallback chain takes precedence."
            )
        # Return a config that signals fallback chain execution
        return ExtendedExecutionConfig(
            base=ExecutionConfig(
                mode=ExecutionMode.SEQUENCE,  # Fallback is sequential failover
                target_backends=None,
                max_backends=1,
                timeout_secs=float(getattr(config, "FALLBACK_TOTAL_TIMEOUT_SECS", 120))
            ),
            fallback_chain=fallback_chain
        )

    # Check header first, then query param
    mode_str = request.headers.get("x-execution-mode") or request.query_params.get("mode")
    if not mode_str:
        return None

    mode_str = mode_str.lower().strip()
    try:
        mode = ExecutionMode(mode_str)
    except ValueError:
        return None

    # Parse target backends
    targets_str = request.headers.get("x-target-backends") or request.query_params.get("backends")
    target_backends = None
    if targets_str:
        target_backends = [t.strip() for t in targets_str.split(",") if t.strip()]

    # Parse max backends
    max_str = request.headers.get("x-max-backends") or request.query_params.get("max_backends")
    max_backends = int(getattr(config, "MULTI_EXEC_MAX_BACKENDS", 3))
    if max_str:
        try:
            max_backends = int(max_str)
        except ValueError:
            pass

    # Parse timeout
    timeout_str = request.headers.get("x-execution-timeout")
    timeout_secs = float(getattr(config, "MULTI_EXEC_TIMEOUT_SECS", 60.0))
    if timeout_str:
        try:
            timeout_secs = float(timeout_str)
        except ValueError:
            pass

    # Parse and validate oracle header (for consensus mode)
    oracle_raw = request.headers.get("x-consensus-oracle")
    oracle = None
    oracle_valid = True

    if oracle_raw and mode == ExecutionMode.CONSENSUS:
        # Normalize: "openai" -> "cloud:openai", "cloud:openai" -> "cloud:openai"
        oracle_name = oracle_raw.strip().replace("cloud:", "")
        cloud_backends = getattr(config, "CLOUD_BACKENDS", {}) or {}

        if oracle_name in cloud_backends:
            oracle = f"cloud:{oracle_name}"
        else:
            oracle_valid = False  # Unknown backend key
            logger.warning(f"Invalid oracle backend: {oracle_raw}")

    return ExtendedExecutionConfig(
        base=ExecutionConfig(
            mode=mode,
            target_backends=target_backends,
            max_backends=max_backends,
            timeout_secs=timeout_secs
        ),
        oracle_backend=oracle,
        oracle_valid=oracle_valid
    )


def _resolve_backend_alias(alias_or_host: str) -> str:
    """Resolve a backend alias to host:port, or return as-is if not an alias."""
    aliases = getattr(config, "BACKEND_ALIASES", {}) or {}
    return aliases.get(alias_or_host, alias_or_host)


def _get_backend_alias(host_port: str) -> Optional[str]:
    """Get human-friendly alias for a host:port, or None if not aliased."""
    reverse = getattr(config, "BACKEND_ALIASES_REVERSE", {}) or {}
    return reverse.get(host_port)


def _is_cloud_backend(node: str) -> bool:
    """Check if a node is a cloud backend (node address starts with 'cloud:')."""
    return node.startswith("cloud:")


def _get_cloud_backend_name(node: str) -> Optional[str]:
    """Extract cloud backend name from node address (e.g., 'cloud:openai' -> 'openai')."""
    if not _is_cloud_backend(node):
        return None
    return node.split(":", 1)[1] if ":" in node else None


def _get_cloud_config(node: str) -> Optional[Dict[str, Any]]:
    """Get cloud backend configuration for a node."""
    name = _get_cloud_backend_name(node)
    if not name:
        return None
    cloud_backends = getattr(config, "CLOUD_BACKENDS", {}) or {}
    return cloud_backends.get(name)


def _get_cloud_url(node: str, endpoint: str = "/chat/completions") -> Optional[str]:
    """Get the full URL for a cloud backend endpoint.

    Uses provider adapters to determine the correct endpoint path.

    Args:
        node: Cloud node address (e.g., 'cloud:openai')
        endpoint: API endpoint path (e.g., '/chat/completions')

    Returns:
        Full URL (e.g., 'https://api.openai.com/v1/chat/completions') or None
    """
    cfg = _get_cloud_config(node)
    if not cfg:
        return None
    base_url = cfg.get("url", "").rstrip("/")
    if not base_url:
        return None

    # Use provider adapter for endpoint path
    provider_type = cfg.get("provider_type", "openai")
    adapter = get_adapter(provider_type)

    # Map generic endpoint to provider-specific endpoint
    if endpoint == "/chat/completions":
        endpoint = adapter.get_chat_endpoint()
    elif endpoint == "/embeddings":
        endpoint = adapter.get_embeddings_endpoint()

    return f"{base_url}{endpoint}"


def _get_cloud_headers(node: str, headers: Dict[str, str]) -> Dict[str, str]:
    """Inject cloud API auth into headers for a cloud backend request.

    Uses provider adapters for provider-specific auth (Bearer vs x-api-key).

    Returns updated headers dict with authentication set.
    """
    cfg = _get_cloud_config(node)
    if not cfg:
        return headers

    api_key = cfg.get("api_key", "")
    if not api_key:
        return headers

    # Use provider adapter for headers
    provider_type = cfg.get("provider_type", "openai")
    adapter = get_adapter(provider_type)

    return adapter.get_headers(api_key, headers)


def _get_equivalent_models(model_name: str) -> List[str]:
    """Get all equivalent model names for a given model.

    If model_name is a canonical name in MODEL_EQUIVALENTS, returns the list.
    If model_name is in MODEL_EQUIVALENTS_REVERSE, returns all equivalents for that group.
    Otherwise returns [model_name].
    """
    equivalents = getattr(config, "MODEL_EQUIVALENTS", {}) or {}
    reverse = getattr(config, "MODEL_EQUIVALENTS_REVERSE", {}) or {}

    # Check if it's a canonical name
    if model_name in equivalents:
        return equivalents[model_name]

    # Check if it's in an equivalence group
    if model_name in reverse:
        canonical = reverse[model_name]
        return equivalents.get(canonical, [model_name])

    return [model_name]


async def _get_backend_model(backend: str, requested_model: str) -> Optional[str]:
    """Find which model name a backend actually has that matches the requested model.

    Returns the actual model ID to use for this backend, or None if no match.
    """
    equivalent_models = _get_equivalent_models(requested_model)
    # Cloud backends: choose an appropriate cloud model for the requested model.
    if _is_cloud_backend(backend):
        cloud_name = _get_cloud_backend_name(backend)
        cloud_models = getattr(config, "CLOUD_MODELS", {}) or {}
        allowed = cloud_models.get(cloud_name, []) if cloud_name else None

        # If there is no explicit CLOUD_MODELS entry, fall back to requested model.
        if not allowed:
            return requested_model

        # Exact/equivalent match first
        if requested_model in allowed:
            return requested_model
        for equiv in equivalent_models:
            if equiv in allowed:
                return equiv

        # Cross-model: pick by class
        if bool(getattr(config, "CROSS_MODEL_FALLBACK", 0)):
            cls = _get_model_class(requested_model)
            picked = _pick_cloud_model_for_class(cls, cloud_name)
            return picked

        return None


    try:
        models_json = await redis_client.get(f"node:{backend}:models")
        if not models_json:
            return None
        models = json.loads(models_json).get("data", [])
        backend_model_ids = {m.get("id") for m in models if m.get("id")}

        # Check for exact match first
        if requested_model in backend_model_ids:
            return requested_model

        # Check for equivalent matches
        for equiv in equivalent_models:
            if equiv in backend_model_ids:
                return equiv

        return None
    except Exception:
        return None


async def _get_eligible_nodes_for_equivalents(model_name: str) -> List[str]:
    """Get eligible nodes that have the requested model OR any equivalent model."""
    equivalent_models = _get_equivalent_models(model_name)

    # If no equivalents defined, use standard eligibility
    if len(equivalent_models) <= 1 and model_name not in getattr(config, "MODEL_EQUIVALENTS", {}):
        return await get_eligible_nodes(model_name)

    # Gather eligible nodes for all equivalent models
    all_eligible = set()
    for equiv_model in equivalent_models:
        nodes = await get_eligible_nodes(equiv_model)
        all_eligible.update(nodes)

    # Also check the canonical name if different
    if model_name not in equivalent_models:
        nodes = await get_eligible_nodes(model_name)
        all_eligible.update(nodes)

    return list(all_eligible)


async def _backend_supports_model(backend: str, model_name: str) -> bool:
    """Check if backend supports the requested model.

    For cloud backends, checks CLOUD_MODELS configuration.
    For local backends, checks Redis model list.
    """
    if _is_cloud_backend(backend):
        # Check cloud models config
        cloud_name = _get_cloud_backend_name(backend)
        cloud_models = getattr(config, "CLOUD_MODELS", {}) or {}
        if cloud_name and cloud_name in cloud_models:
            model_list = cloud_models[cloud_name]
            # Check exact match or equivalents
            equivalent_models = _get_equivalent_models(model_name)
            result = model_name in model_list or any(m in model_list for m in equivalent_models)

            # Cross-model fallback: if enabled, allow cloud backend to participate in consensus
            # by selecting an appropriate cloud model from the same model class.
            if not result and bool(getattr(config, "CROSS_MODEL_FALLBACK", 0)):
                cls = _get_model_class(model_name)
                picked = _pick_cloud_model_for_class(cls, cloud_name)
                if picked:
                    result = True

            logger.info(
                "cloud_support_check: backend=%s model=%s cloud_name=%s models=%s equivalents=%s result=%s",
                backend,
                model_name,
                cloud_name,
                model_list,
                equivalent_models,
                result,
            )
            return result
        # If no config, assume cloud backends support requested model
        return True

    # For local backends, check Redis
    try:
        models_json = await redis_client.get(f"node:{backend}:models")
        if not models_json:
            return False
        models = json.loads(models_json).get("data", [])
        backend_model_ids = {m.get("id") for m in models if m.get("id")}
        # Check exact match or equivalents
        equivalent_models = _get_equivalent_models(model_name)
        return model_name in backend_model_ids or any(m in backend_model_ids for m in equivalent_models)
    except Exception:
        return False


async def _get_eligible_cloud_backends(model_name: str) -> List[str]:
    """Get list of cloud backends that support the requested model and are not rate-limited."""
    cloud_backends = getattr(config, "CLOUD_BACKENDS", {}) or {}
    eligible = []
    for name in cloud_backends.keys():
        cloud_node = f"cloud:{name}"
        # Check rate limit status
        if await _is_rate_limited(cloud_node):
            continue
        # Check model support
        if await _backend_supports_model(cloud_node, model_name):
            eligible.append(cloud_node)
    logger.info("eligible_cloud_backends: model=%s eligible=%s", model_name, eligible)
    return eligible


async def _select_backends_for_consensus(
    exec_config: ExecutionConfig,
    model_name: str,
    eligible_local_nodes: List[str],
    oracle_backend: Optional[str] = None
) -> Tuple[List[str], bool]:
    """Select backends ensuring mix of local + cloud for consensus.

    Mix Policy:
    - Minimum: 1 local + 1 cloud (if both available)
    - Target: 2 local + 1 cloud for 3-way consensus
    - Fallback: if only one side exists, use what's available

    Args:
        exec_config: Execution configuration
        model_name: The requested model name
        eligible_local_nodes: List of eligible local nodes
        oracle_backend: Optional oracle backend to force-include

    Returns:
        (selected_backends, oracle_present)
    """
    # Get eligible cloud backends
    cloud_capable = await _get_eligible_cloud_backends(model_name)
    local_capable = [n for n in eligible_local_nodes if not _is_cloud_backend(n)]
    logger.info(
        "consensus_select: model=%s local_capable=%s cloud_capable=%s cross_model=%s",
        model_name,
        local_capable,
        cloud_capable,
        getattr(config, "CROSS_MODEL_FALLBACK", 0),
    )

    # Force-include oracle if specified and capable
    oracle_present = False
    selected = []

    if oracle_backend and oracle_backend in cloud_capable:
        selected.append(oracle_backend)
        cloud_capable.remove(oracle_backend)
        oracle_present = True
    elif oracle_backend and oracle_backend not in cloud_capable:
        # Oracle was requested but not available
        logger.warning(f"Oracle backend {oracle_backend} not available for model {model_name}")

    # Apply mix policy
    max_b = exec_config.max_backends
    remaining = max_b - len(selected)

    if local_capable and cloud_capable:
        # Both sides available: aim for 2 local + 1 cloud (or 1+1 minimum)
        local_target = min(2, remaining - 1) if remaining > 1 else 1
        cloud_target = 1 if not oracle_present else 0
    elif local_capable:
        # Only local available
        local_target = remaining
        cloud_target = 0
    else:
        # Only cloud available
        local_target = 0
        cloud_target = remaining

    # Select local backends
    local_pool = list(local_capable)
    for _ in range(min(local_target, len(local_pool))):
        if not local_pool:
            break
        # Use router to pick best candidate from remaining pool
        candidate = await router.select_node(local_pool, model_name, redis_client)
        if candidate:
            selected.append(candidate)
            local_pool.remove(candidate)

    # Select additional cloud backends
    for _ in range(min(cloud_target, len(cloud_capable))):
        if not cloud_capable:
            break
        candidate = cloud_capable.pop(0)  # Take first available
        selected.append(candidate)

    return selected, oracle_present


async def _select_backends_for_execution(
    exec_config: ExecutionConfig,
    model_name: str,
    eligible_nodes: List[str]
) -> List[str]:
    """Select backends for multi-exec based on config.

    If target_backends specified, resolve aliases and filter to eligible.
    Otherwise, select up to max_backends from eligible pool.
    """
    if exec_config.target_backends:
        # Resolve aliases and filter to eligible
        resolved = [_resolve_backend_alias(t) for t in exec_config.target_backends]
        # Keep order but filter to eligible
        return [b for b in resolved if b in eligible_nodes][:exec_config.max_backends]

    # Auto-select from eligible pool
    # Use router to select diverse backends
    selected = []
    pool = list(eligible_nodes)

    for _ in range(min(exec_config.max_backends, len(pool))):
        if not pool:
            break
        # Use router to pick best candidate from remaining pool
        candidate = await router.select_node(pool, model_name, redis_client)
        if candidate:
            selected.append(candidate)
            pool.remove(candidate)

    return selected


async def _make_backend_request(
    backend: str,
    body: Dict[str, Any],
    headers: Dict[str, str],
    model_name: str,
    request_id: str,
    start_time: float,
    backend_model_override: Optional[str] = None
) -> BackendResult:
    """Make a single non-streaming request to a backend and return result.

    For cloud backends, uses provider adapters for request/response transformation.
    """
    # Determine URL and adapter based on backend type (cloud vs local)
    adapter = None
    if _is_cloud_backend(backend):
        url = _get_cloud_url(backend, "/chat/completions")
        if not url:
            return BackendResult(
                backend=backend,
                alias=_get_cloud_backend_name(backend),
                error="Cloud backend URL not configured"
            )
        # Inject API key for cloud backends
        headers = _get_cloud_headers(backend, headers)
        alias = _get_cloud_backend_name(backend)
        # Get provider adapter for request/response transformation
        cfg = _get_cloud_config(backend)
        if cfg:
            provider_type = cfg.get("provider_type", "openai")
            adapter = get_adapter(provider_type)
    else:
        url = f"http://{backend}/v1/chat/completions"
        alias = _get_backend_alias(backend)

    result = BackendResult(backend=backend, alias=alias)
    t0 = time.monotonic()

    # Use override model if provided (for model equivalence support)
    actual_model = backend_model_override or model_name
    request_body = dict(body)
    if backend_model_override:
        request_body["model"] = backend_model_override

    # Transform request for provider-specific format
    if adapter:
        request_body = adapter.transform_request(request_body)

    acquired = await _acquire_slot(backend)
    if not acquired:
        result.error = "Backend at capacity"
        result.latency_ms = (time.monotonic() - t0) * 1000
        return result

    model_ok = await _acquire_model_slot(actual_model)
    if not model_ok:
        await _dec_inflight(backend)
        result.error = "Model at capacity"
        result.latency_ms = (time.monotonic() - t0) * 1000
        return result

    try:
        try:
            resp = await http_client.post(url, json=request_body, headers=headers)
            result.status_code = resp.status_code
            result.first_byte_ms = (time.monotonic() - t0) * 1000

            # Handle rate limiting (429)
            if resp.status_code == 429:
                # Extract Retry-After header if present
                retry_after = resp.headers.get("retry-after")
                retry_after_secs = None
                if retry_after:
                    try:
                        retry_after_secs = float(retry_after)
                    except ValueError:
                        pass
                await _record_rate_limit(backend, retry_after_secs)
                result.error = f"HTTP 429 Rate Limited"
            elif resp.status_code >= 500 or resp.status_code == 404:
                await _record_failure(backend)
                result.error = f"HTTP {resp.status_code}"
            else:
                await _record_success(backend)
                # Clear any previous rate limit state on success
                await _clear_rate_limit(backend)
                result.success = True
                try:
                    response_body = resp.json()
                    # Transform response to OpenAI format if using provider adapter
                    if adapter:
                        response_body = adapter.transform_response(response_body)
                    result.response_body = response_body
                except Exception:
                    result.response_body = {"raw": resp.text}
        except AttributeError:
            # Compatibility for test fakes that only implement .stream()
            async with http_client.stream("POST", url, json=request_body, headers=headers) as response:
                result.status_code = getattr(response, "status_code", 200)
                result.first_byte_ms = (time.monotonic() - t0) * 1000

                # Handle rate limiting (429)
                if result.status_code == 429:
                    retry_after = getattr(response, "headers", {}).get("retry-after")
                    retry_after_secs = None
                    if retry_after:
                        try:
                            retry_after_secs = float(retry_after)
                        except ValueError:
                            pass
                    await _record_rate_limit(backend, retry_after_secs)
                    result.error = f"HTTP 429 Rate Limited"
                elif result.status_code >= 500 or result.status_code == 404:
                    await _record_failure(backend)
                    result.error = f"HTTP {result.status_code}"
                else:
                    chunks = bytearray()
                    async for chunk in response.aiter_bytes():
                        chunks.extend(chunk)
                    await _record_success(backend)
                    await _clear_rate_limit(backend)
                    result.success = True
                    try:
                        response_body = json.loads(bytes(chunks))
                        # Transform response to OpenAI format if using provider adapter
                        if adapter:
                            response_body = adapter.transform_response(response_body)
                        result.response_body = response_body
                    except Exception:
                        result.response_body = {"raw": bytes(chunks).decode(errors="replace")}

    except httpx.RequestError as e:
        await _record_failure(backend)
        result.error = str(e)
    except Exception as e:
        await _record_failure(backend)
        result.error = str(e)
    finally:
        await _dec_inflight(backend)
        await _dec_model(model_name)
        result.latency_ms = (time.monotonic() - t0) * 1000

    return result


async def _record_multi_exec_metrics(mode: str, backends_attempted: int, backends_succeeded: int):
    """Record metrics for multi-backend execution."""
    try:
        await redis_client.incrby(f"lb:multi_exec_total:{mode}", 1)
        await redis_client.incrby("lb:multi_exec_total", 1)
        await redis_client.incrby(f"lb:multi_exec_backends_sum:{mode}", backends_attempted)
        await redis_client.incrby(f"lb:multi_exec_backends_count:{mode}", 1)
        await redis_client.incrby(f"lb:multi_exec_succeeded_sum:{mode}", backends_succeeded)
    except Exception:
        pass


async def _record_consensus_metrics(
    model_name: str,
    consensus: ConsensusResult,
    backends: List[str],
    oracle_valid: bool = True
):
    """Record detailed metrics for consensus operations.

    Tracks:
    - Agreement counts (how many backends agreed)
    - Disagreement rate (when not unanimous)
    - Comparison type distribution (hash, text, tool_calls)
    - Per-model consensus quality
    - Backend combinations that disagree
    - Local vs cloud agreement
    - Oracle tracking
    - Per-backend error tracking
    """
    try:
        # Determine model class for per-class tracking
        model_class = _get_model_class(model_name)

        # Total consensus requests
        await redis_client.incrby("lb:consensus_total", 1)
        await redis_client.incrby(f"lb:consensus_total:{model_name}", 1)

        # Agreement count histogram
        agreement_count = consensus.agreement_count
        await redis_client.incrby(f"lb:consensus_agreement:{agreement_count}", 1)
        await redis_client.incrby(f"lb:consensus_agreement:{model_name}:{agreement_count}", 1)

        # Disagreement tracking
        if consensus.disagreement:
            await redis_client.incrby("lb:consensus_disagreements", 1)
            await redis_client.incrby(f"lb:consensus_disagreements:{model_name}", 1)

            # Track which backend combination disagreed (for debugging)
            backend_key = "|".join(sorted(backends))
            await redis_client.incrby(f"lb:consensus_disagreements:backends:{backend_key}", 1)
        else:
            await redis_client.incrby("lb:consensus_agreements", 1)
            await redis_client.incrby(f"lb:consensus_agreements:{model_name}", 1)

        # Comparison type distribution
        comp_type = consensus.comparison_type
        await redis_client.incrby(f"lb:consensus_comparison:{comp_type}", 1)
        await redis_client.incrby(f"lb:consensus_comparison:{model_name}:{comp_type}", 1)

        # Add to series for easier metric export
        await redis_client.sadd("lb:consensus_models", model_name)

        # Local vs cloud agreement tracking
        local_backends = [b for b in backends if not _is_cloud_backend(b)]
        cloud_backends = [b for b in backends if _is_cloud_backend(b)]

        if local_backends and cloud_backends:
            await redis_client.incrby("lb:consensus_local_cloud_total", 1)
            if model_class:
                await redis_client.incrby(f"lb:consensus_local_cloud_total:{model_class}", 1)

            if consensus.local_cloud_agreement is True:
                await redis_client.incrby("lb:consensus_local_cloud_agreed", 1)
                if model_class:
                    await redis_client.incrby(f"lb:consensus_local_cloud_agreed:{model_class}", 1)
            elif consensus.local_cloud_agreement is False:
                await redis_client.incrby("lb:consensus_local_cloud_disagreed", 1)

            # Track similarity score histogram (buckets: 0.0, 0.1, ..., 1.0)
            if consensus.local_cloud_similarity is not None:
                bucket = int(consensus.local_cloud_similarity * 10) / 10  # Round to nearest 0.1
                await redis_client.incrby(f"lb:consensus_similarity_bucket:{bucket:.1f}", 1)

        # Oracle tracking
        if consensus.oracle_backend:
            await redis_client.incrby("lb:consensus_oracle_requested", 1)
            if consensus.oracle_present:
                await redis_client.incrby("lb:consensus_oracle_present", 1)
                if consensus.oracle_agreed is True:
                    await redis_client.incrby("lb:consensus_oracle_agreed", 1)
                elif consensus.oracle_agreed is False:
                    await redis_client.incrby("lb:consensus_oracle_disagreed", 1)
            else:
                await redis_client.incrby("lb:consensus_oracle_missing", 1)

        if not oracle_valid:
            await redis_client.incrby("lb:consensus_oracle_invalid", 1)

        # Per-backend error code tracking
        for result in consensus.all_responses:
            if result.status_code == 429:
                await redis_client.incrby(f"lb:backend_errors:429:{result.backend}", 1)
            elif result.status_code >= 500:
                await redis_client.incrby(f"lb:backend_errors:5xx:{result.backend}", 1)

    except Exception:
        pass


def _get_model_class(model_name: str) -> Optional[str]:
    """Get the model class (e.g., 'small', 'medium', 'large') for a model."""
    classes = getattr(config, "MODEL_CLASSES", {}) or {}
    for class_name, class_cfg in classes.items():
        candidates = class_cfg.get("candidates", [])
        if model_name in candidates:
            return class_name
    return None



def _pick_cloud_model_for_class(model_class: Optional[str], cloud_backend_name: str) -> Optional[str]:
    """Pick a cloud model id for a given model class and cloud backend.

    Preference order:
    1) First candidate in MODEL_CLASSES[model_class].candidates that is listed for this cloud backend.
    2) Otherwise, first configured CLOUD_MODELS entry for the backend.

    Returns None if no candidate exists.
    """
    cloud_models = getattr(config, "CLOUD_MODELS", {}) or {}
    allowed = cloud_models.get(cloud_backend_name, []) or []
    if not allowed:
        return None

    if model_class:
        classes_cfg = getattr(config, "MODEL_CLASSES", {}) or {}
        candidates = ((classes_cfg.get(model_class) or {}).get("candidates", []) or [])
        for m in candidates:
            if m in allowed:
                return m

    return allowed[0]


async def _handle_multi_backend_execution(
    request: Request,
    body: Dict[str, Any],
    ext_config: ExtendedExecutionConfig,
    model_name: str,
    eligible_nodes: List[str],
    request_id: str,
    start_time: float,
    is_stream: bool,
    on_demand_wait: bool,
    warm_wait_ms: int,
    model_defaulted: bool
) -> Response:
    """Handle multi-backend execution modes (race, all, sequence, consensus).

    Supports extended execution config with oracle and fallback chain features.
    """
    exec_config = ext_config.base
    oracle_backend = ext_config.oracle_backend
    oracle_valid = ext_config.oracle_valid

    headers = {
        key: value
        for key, value in request.headers.items()
        if key.lower() not in ('host', 'content-length', 'x-execution-mode', 'x-target-backends', 'x-max-backends', 'x-consensus-oracle', 'x-fallback-chain')
    }
    headers["x-request-id"] = request_id

    # Expand eligible nodes to include those with equivalent models
    expanded_eligible = await _get_eligible_nodes_for_equivalents(model_name)
    if expanded_eligible:
        # Merge with original eligible_nodes (union)
        all_eligible = list(set(eligible_nodes) | set(expanded_eligible))
    else:
        all_eligible = eligible_nodes

    # Select backends - use consensus-specific selection for consensus mode
    oracle_present = False
    if exec_config.mode == ExecutionMode.CONSENSUS:
        backends, oracle_present = await _select_backends_for_consensus(
            exec_config, model_name, all_eligible, oracle_backend
        )
    else:
        backends = await _select_backends_for_execution(exec_config, model_name, all_eligible)

    if not backends:
        raise HTTPException(status_code=404, detail="No eligible backends for multi-exec")

    # Build mapping of backend -> actual model name to use
    backend_models: Dict[str, str] = {}
    for backend in backends:
        actual_model = await _get_backend_model(backend, model_name)
        backend_models[backend] = actual_model or model_name

    # Create request factory with per-backend model override
    async def make_request(backend: str) -> BackendResult:
        backend_model = backend_models.get(backend, model_name)
        return await _make_backend_request(
            backend, body, headers, model_name, request_id, start_time,
            backend_model_override=backend_model if backend_model != model_name else None
        )

    engine = ExecutionEngine(
        similarity_threshold=float(getattr(config, "MULTI_EXEC_CONSENSUS_THRESHOLD", 0.9))
    )

    await _inc_requests_total()

    # Non-streaming multi-exec
    if exec_config.mode == ExecutionMode.RACE:
        result = await engine.execute_race(backends, make_request, exec_config.timeout_secs)
        await _record_multi_exec_metrics("race", len(backends), 1 if result.success else 0)

        if not result.success:
            raise HTTPException(status_code=502, detail=result.error or "All backends failed")

        resp_body = result.response_body or {}
        out = JSONResponse(content=resp_body)

    elif exec_config.mode == ExecutionMode.ALL:
        results = await engine.execute_all(backends, make_request, exec_config.timeout_secs)
        succeeded = sum(1 for r in results if r.success)
        await _record_multi_exec_metrics("all", len(backends), succeeded)

        # Build response with all results
        resp_body = {
            "mode": "all",
            "backends_attempted": len(backends),
            "backends_succeeded": succeeded,
            "responses": [
                {
                    "backend": r.backend,
                    "alias": r.alias,
                    "success": r.success,
                    "latency_ms": r.latency_ms,
                    "error": r.error,
                    "response": r.response_body
                }
                for r in results
            ]
        }
        out = JSONResponse(content=resp_body)

    elif exec_config.mode == ExecutionMode.SEQUENCE:
        results = await engine.execute_sequence(backends, make_request, exec_config.timeout_secs, stop_on_success=False)
        succeeded = sum(1 for r in results if r.success)
        await _record_multi_exec_metrics("sequence", len(results), succeeded)

        resp_body = {
            "mode": "sequence",
            "backends_attempted": len(results),
            "backends_succeeded": succeeded,
            "responses": [
                {
                    "backend": r.backend,
                    "alias": r.alias,
                    "success": r.success,
                    "latency_ms": r.latency_ms,
                    "error": r.error,
                    "response": r.response_body
                }
                for r in results
            ]
        }
        out = JSONResponse(content=resp_body)

    elif exec_config.mode == ExecutionMode.CONSENSUS:
        # Execute consensus with oracle and local/cloud tracking
        consensus = await engine.execute_consensus(
            backends, make_request, exec_config.timeout_secs,
            oracle_backend=oracle_backend,
            oracle_present=oracle_present,
            is_cloud_fn=_is_cloud_backend
        )
        succeeded = sum(1 for r in consensus.all_responses if r.success)
        await _record_multi_exec_metrics("consensus", len(backends), succeeded)
        await _record_consensus_metrics(model_name, consensus, backends, oracle_valid)

        # Build consensus response
        resp_body = {
            "mode": "consensus",
            "winner": {
                "backend": consensus.winner.backend,
                "alias": consensus.winner.alias,
                "response": consensus.winner.response_body,
                "latency_ms": consensus.winner.latency_ms
            },
            "agreement_count": consensus.agreement_count,
            "disagreement": consensus.disagreement,
            "comparison_type": consensus.comparison_type,
            "oracle_backend": consensus.oracle_backend,
            "oracle_present": consensus.oracle_present,
            "oracle_agreed": consensus.oracle_agreed,
            "local_cloud_agreement": consensus.local_cloud_agreement,
            "local_cloud_similarity": consensus.local_cloud_similarity,
            "all_responses": [
                {
                    "backend": r.backend,
                    "alias": r.alias,
                    "success": r.success,
                    "latency_ms": r.latency_ms,
                    "error": r.error,
                    "response": r.response_body
                }
                for r in consensus.all_responses
            ]
        }
        out = JSONResponse(content=resp_body)

        # Add consensus-specific headers
        if consensus.disagreement:
            out.headers["x-disagreement"] = "true"

        # Oracle validation headers
        if not oracle_valid:
            out.headers["x-consensus-oracle"] = "invalid"
        elif consensus.oracle_backend:
            out.headers["x-consensus-oracle"] = _get_cloud_backend_name(consensus.oracle_backend) or consensus.oracle_backend
            out.headers["x-oracle-present"] = "true" if consensus.oracle_present else "false"
            if consensus.oracle_present and consensus.oracle_agreed is not None:
                out.headers["x-oracle-agreed"] = "true" if consensus.oracle_agreed else "false"

        # Local vs cloud agreement headers
        if consensus.local_cloud_agreement is not None:
            out.headers["x-local-cloud-agreement"] = "true" if consensus.local_cloud_agreement else "false"
        if consensus.local_cloud_similarity is not None:
            out.headers["x-local-cloud-similarity"] = f"{consensus.local_cloud_similarity:.2f}"

    else:
        raise HTTPException(status_code=400, detail=f"Unknown execution mode: {exec_config.mode}")

    # Common headers
    out.headers["x-execution-mode"] = exec_config.mode.value
    out.headers["x-backends-attempted"] = ",".join(backends)
    out.headers["x-backends-count"] = str(len(backends))
    out.headers["x-selected-model"] = model_name
    out.headers["x-request-id"] = request_id
    out.headers["x-on-demand-wait"] = "true" if on_demand_wait else "false"
    out.headers["x-warm-wait-ms"] = str(warm_wait_ms)
    out.headers["x-model-defaulted"] = "true" if model_defaulted else "false"

    # Find fastest backend
    if exec_config.mode in (ExecutionMode.ALL, ExecutionMode.SEQUENCE, ExecutionMode.CONSENSUS):
        try:
            if exec_config.mode == ExecutionMode.CONSENSUS:
                all_results = consensus.all_responses
            else:
                all_results = results
            successful = [r for r in all_results if r.success]
            if successful:
                fastest = min(successful, key=lambda r: r.latency_ms)
                out.headers["x-fastest-backend"] = fastest.alias or fastest.backend
        except Exception:
            pass

    return out


async def _execute_with_fallback_chain(
    chain_name: str,
    body: Dict[str, Any],
    headers: Dict[str, str],
    request_id: str,
    start_time: float,
    model_name: str
) -> Response:
    """Execute request following a fallback chain until success.

    Each backend in the chain has its own timeout and retry budget.
    Total execution respects FALLBACK_TOTAL_TIMEOUT_SECS.

    Args:
        chain_name: Name of the fallback chain from config
        body: Request body
        headers: Request headers
        request_id: Request ID for tracking
        start_time: Request start time
        model_name: The requested model name

    Returns:
        JSONResponse with the successful result

    Raises:
        HTTPException if all backends in chain fail
    """
    chains = getattr(config, "FALLBACK_CHAINS", {}) or {}
    chain = chains.get(chain_name, [])

    if not chain:
        raise HTTPException(status_code=400, detail=f"Unknown fallback chain: {chain_name}")

    total_timeout = float(getattr(config, "FALLBACK_TOTAL_TIMEOUT_SECS", 120))
    chain_start = time.monotonic()
    last_error = None
    attempted_backends: List[str] = []
    last_status_code = 502

    await _inc_requests_total()

    for fb in chain:
        # Check total timeout
        elapsed = time.monotonic() - chain_start
        if elapsed >= total_timeout:
            logger.warning(f"[req={request_id}] Fallback chain {chain_name} exceeded total timeout")
            break

        backend = fb.backend
        remaining_timeout = min(fb.timeout_secs, total_timeout - elapsed)

        # Resolve "local:auto" to best available local backend
        if backend == "local:auto":
            local_nodes = await get_eligible_nodes(model_name)
            local_nodes = [n for n in local_nodes if not _is_cloud_backend(n)]
            if not local_nodes:
                logger.info(f"[req={request_id}] No local backends available, skipping local:auto")
                continue
            backend = await router.select_node(local_nodes, model_name, redis_client)
            if not backend:
                continue

        # Normalize cloud backend names
        if not _is_cloud_backend(backend) and not ":" in backend:
            # Might be a cloud backend name without prefix
            cloud_backends = getattr(config, "CLOUD_BACKENDS", {}) or {}
            if backend in cloud_backends:
                backend = f"cloud:{backend}"

        # Per-backend retry loop
        for attempt in range(fb.max_retries + 1):
            try:
                attempted_backends.append(backend)
                logger.info(f"[req={request_id}] Fallback chain {chain_name}: trying {backend} (attempt {attempt + 1})")

                result = await asyncio.wait_for(
                    _make_backend_request(
                        backend, body, headers, model_name, request_id, start_time
                    ),
                    timeout=remaining_timeout
                )

                if result.success:
                    logger.info(f"[req={request_id}] Fallback chain {chain_name}: success on {backend}")
                    # Record metrics
                    await redis_client.incrby(f"lb:fallback_chain_success:{chain_name}", 1)
                    await redis_client.incrby(f"lb:fallback_chain_success:{chain_name}:{backend}", 1)

                    out = JSONResponse(content=result.response_body)
                    out.headers["x-fallback-chain"] = chain_name
                    out.headers["x-fallback-backend"] = backend
                    out.headers["x-fallback-attempts"] = str(len(attempted_backends))
                    out.headers["x-request-id"] = request_id
                    out.headers["x-selected-model"] = model_name
                    return out

                last_error = result.error
                last_status_code = result.status_code or 502

                # Don't retry on 4xx (client errors) except 429
                if result.status_code and 400 <= result.status_code < 500:
                    if result.status_code == 429:
                        # Check if we have Retry-After that's within our budget
                        # The rate limit is already recorded, so this backend will be skipped
                        # on next iteration. Move to next backend in chain.
                        logger.info(f"[req={request_id}] Backend {backend} rate limited (429), moving to next")
                        break
                    # Other 4xx - client error, don't retry
                    logger.info(f"[req={request_id}] Backend {backend} returned {result.status_code}, moving to next")
                    break

            except asyncio.TimeoutError:
                last_error = f"Timeout after {remaining_timeout}s"
                logger.info(f"[req={request_id}] Backend {backend} timed out")
                break
            except Exception as e:
                last_error = str(e)
                logger.warning(f"[req={request_id}] Backend {backend} error: {e}")

            # Backoff between retries (only for 5xx/network errors)
            if attempt < fb.max_retries:
                backoff = 0.5 * (attempt + 1)
                await asyncio.sleep(backoff)

    # All backends failed
    logger.warning(f"[req={request_id}] Fallback chain {chain_name} exhausted. Last error: {last_error}")
    await redis_client.incrby(f"lb:fallback_chain_exhausted:{chain_name}", 1)

    raise HTTPException(
        status_code=last_status_code,
        detail={
            "message": f"All backends in chain '{chain_name}' failed",
            "last_error": last_error,
            "attempted": attempted_backends
        },
        headers={"x-fallback-chain": chain_name}
    )


@app.get("/v1/models")
async def get_all_models():
    """Aggregates and de-duplicates model lists from all healthy nodes."""
    healthy_nodes = await redis_client.smembers("nodes:healthy")
    all_models = {}
    for node in healthy_nodes:
        models_json = await redis_client.get(f"node:{node}:models")
        if models_json:
            models = json.loads(models_json).get("data", [])
            for model in models:
                if model['id'] not in all_models:
                    all_models[model['id']] = model
    return {"object": "list", "data": list(all_models.values())}

# Latency histogram buckets in seconds
_LAT_BUCKETS = [0.05, 0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0, float("inf")]

async def _record_latency(model: str, node: str, elapsed_secs: float):
    try:
        series_key = f"{model}|{node}"
        await redis_client.sadd("lb:latency_series", series_key)
        # Increment sum and count
        await redis_client.incrbyfloat(f"lb:latency_sum:{series_key}", float(elapsed_secs))
        await redis_client.incrby(f"lb:latency_count:{series_key}", 1)
        # Cumulative buckets: increment every bucket >= observed
        for le in _LAT_BUCKETS:
            if elapsed_secs <= le:
                key = f"lb:latency_bucket:{series_key}:{le}"
                await redis_client.incrby(key, 1)
    except Exception:
        pass

async def _record_stream_ttfb(model: str, node: str, elapsed_secs: float):
    try:
        series_key = f"{model}|{node}"
        await redis_client.sadd("lb:stream_ttfb_series", series_key)
        await redis_client.incrbyfloat(f"lb:stream_ttfb_sum:{series_key}", float(elapsed_secs))
        await redis_client.incrby(f"lb:stream_ttfb_count:{series_key}", 1)
        for le in _LAT_BUCKETS:
            if elapsed_secs <= le:
                await redis_client.incrby(f"lb:stream_ttfb_bucket:{series_key}:{le}", 1)
    except Exception:
        pass

async def _record_stream_duration(model: str, node: str, elapsed_secs: float):
    try:
        series_key = f"{model}|{node}"
        await redis_client.sadd("lb:stream_duration_series", series_key)
        await redis_client.incrbyfloat(f"lb:stream_duration_sum:{series_key}", float(elapsed_secs))
        await redis_client.incrby(f"lb:stream_duration_count:{series_key}", 1)
        for le in _LAT_BUCKETS:
            if elapsed_secs <= le:
                await redis_client.incrby(f"lb:stream_duration_bucket:{series_key}:{le}", 1)
    except Exception:
        pass

@app.api_route("/v1/chat/completions", methods=["POST"])
async def chat_completions(request: Request):
    """Receives a chat completion request, routes it, and streams the response."""
    try:
        raw_body = await request.json()
    except json.JSONDecodeError as exc:
        logger.warning("Invalid JSON payload for /v1/chat/completions: %s", exc)
        raise HTTPException(status_code=400, detail="Request body must be valid JSON.")
    except Exception as exc:
        logger.warning("Failed to parse JSON payload for /v1/chat/completions: %s", exc)
        raise HTTPException(status_code=400, detail="Request body must be valid JSON.")

    sanitized = sanitize_chat_request(raw_body)
    if sanitized.default_applied:
        logger.info(
            "Chat request missing model; applied default '%s'", sanitized.default_applied
        )
    if sanitized.missing_fields:
        raise HTTPException(
            status_code=400,
            detail={
                "message": "Missing required fields.",
                "missing": sanitized.missing_fields,
            },
        )

    body = sanitized.payload
    model_name = sanitized.model_name
    is_stream = bool(body.get("stream", False))
    model_defaulted = sanitized.default_applied is not None
    start_time = time.monotonic()

    # Resolve auto/default sentinel to a concrete model id
    if _is_model_sentinel(model_name):
        prefer_intersection = request.query_params.get("require_all", "false").lower() in ("1", "true", "yes")
        resolved = await _resolve_auto_model(prefer_intersection=prefer_intersection)
        if not resolved:
            raise HTTPException(status_code=404, detail="No models available for auto selection.")
        body["model"] = resolved
        model_name = resolved

    eligible_nodes = await get_eligible_nodes(model_name)

    # Track on-demand warm/wait for observability headers
    on_demand_wait = False
    warm_wait_ms = 0

    if not eligible_nodes:
        # On-demand warm/wait: try to load the model on a few healthy nodes, then poll briefly
        try:
            t0_warm = time.monotonic()
            waited = await _on_demand_wait_for_model(model_name)
            warm_wait_ms = int((time.monotonic() - t0_warm) * 1000)
            if waited:
                eligible_nodes = waited
                on_demand_wait = True
        except Exception:
            pass
    if not eligible_nodes:
        # Try cross-model fallback if enabled
        fb_model: Optional[str] = None
        if getattr(config, "CROSS_MODEL_FALLBACK", False):
            # If original is auto/sentinel, re-resolve; else try class-sibling then auto
            if _is_model_sentinel(model_name):
                fb_model = await _resolve_auto_model(prefer_intersection=False)
            else:
                fb_model = await _find_fallback_model(model_name)
                if not fb_model:
                    fb_model = await _resolve_auto_model(prefer_intersection=False)
            if fb_model:
                fb_nodes = await get_eligible_nodes(fb_model)
                if fb_nodes:
                    logger.warning("No healthy nodes for '%s'; falling back to '%s'", model_name, fb_model)
                    body["model"] = fb_model
                    model_name = fb_model
                    eligible_nodes = fb_nodes
    request_id = request.headers.get("x-request-id") or str(uuid.uuid4())

    # Check for multi-backend execution mode or fallback chain
    exec_config = _parse_execution_mode(request)

    # Handle fallback chain execution (takes precedence over other modes)
    if exec_config and exec_config.fallback_chain:
        headers = {
            key: value for key, value in request.headers.items()
            if key.lower() not in ('host', 'content-length', 'x-fallback-chain', 'x-execution-mode')
        }
        headers["x-request-id"] = request_id
        return await _execute_with_fallback_chain(
            chain_name=exec_config.fallback_chain,
            body=body,
            headers=headers,
            request_id=request_id,
            start_time=start_time,
            model_name=model_name
        )

    # For multi-exec, try model equivalents if no direct match
    if not eligible_nodes and exec_config:
        equiv_nodes = await _get_eligible_nodes_for_equivalents(model_name)
        if equiv_nodes:
            eligible_nodes = equiv_nodes

    if not eligible_nodes:
        raise HTTPException(status_code=404, detail=f"No healthy nodes found for model '{model_name}'.")
    if exec_config:
        return await _handle_multi_backend_execution(
            request=request,
            body=body,
            ext_config=exec_config,  # ExtendedExecutionConfig with oracle/fallback support
            model_name=model_name,
            eligible_nodes=eligible_nodes,
            request_id=request_id,
            start_time=start_time,
            is_stream=is_stream,
            on_demand_wait=on_demand_wait,
            warm_wait_ms=warm_wait_ms,
            model_defaulted=model_defaulted
        )

    # Standard single-backend path continues below
    headers = {key: value for key, value in request.headers.items() if key.lower() not in ('host', 'content-length')}
    headers["x-request-id"] = request_id
    forced_node = request.query_params.get("node")
    session_id = request.headers.get("x-session-id")

    async def attempt_stream(node: str):
        # Determine URL and headers based on backend type (cloud vs local)
        if _is_cloud_backend(node):
            url = _get_cloud_url(node, "/chat/completions")
            if not url:
                raise httpx.RequestError(f"Cloud backend URL not configured for {node}")
            req_headers = _get_cloud_headers(node, headers)
        else:
            url = f"http://{node}/v1/chat/completions"
            req_headers = headers

        logger.info("[req=%s] Routing request for model '%s' to %s", request_id, model_name, node)
        acquired = await _acquire_slot(node)
        if not acquired:
            logger.info("[req=%s] Node %s at capacity; skipping", request_id, node)
            raise CapacityError("node")
        model_ok = await _acquire_model_slot(model_name)
        if not model_ok:
            await _dec_inflight(node)
            raise CapacityError("model")
        try:
            async with http_client.stream("POST", url, json=body, headers=req_headers) as response:
                # Handle rate limiting (429)
                if response.status_code == 429:
                    retry_after = response.headers.get("retry-after")
                    retry_after_secs = None
                    if retry_after:
                        try:
                            retry_after_secs = float(retry_after)
                        except ValueError:
                            pass
                    await _record_rate_limit(node, retry_after_secs)
                    raise RateLimitError(node, retry_after_secs)
                # Treat 5xx and 404 as retryable failure across nodes
                if response.status_code and (response.status_code >= 500 or response.status_code == 404):
                    await _record_failure(node)
                    raise httpx.HTTPStatusError("Upstream retryable error", request=response.request, response=response)
                async for chunk in response.aiter_bytes():
                    yield chunk
            await _record_success(node)
            await _clear_rate_limit(node)
        finally:
            await _dec_inflight(node)
            await _dec_model(model_name)

    async def stream_with_failover(nodes: List[str]):
        attempts = 0
        tried = set()
        tried_order = []
        last_error = None
        all_capacity = True
        first_byte_recorded = False
        # We allow at most first try + MAX_RETRIES additional
        budget = min(len(nodes), 1 + config.MAX_RETRIES)
        while attempts < budget:
            # prefer nodes without open circuit
            remaining = [n for n in nodes if n not in tried]
            closed = []
            for n in remaining:
                if not await _is_circuit_open(n):
                    closed.append(n)
            pool = closed if closed else remaining  # half-open if all open
            # Sticky preference on first pick
            candidate = None
            if attempts == 0 and not forced_node:
                sticky = await _get_sticky_node(session_id, model_name)
                if sticky and sticky in pool:
                    candidate = sticky
            if candidate is None:
                candidate = await router.select_node(pool, model_name, redis_client)
            if candidate is None:
                break
            tried.add(candidate)
            tried_order.append(candidate)
            attempts += 1
            # Emit SSE meta for initial attempt or failover
            meta = {
                "request_id": request_id,
                "model": model_name,
                "node": candidate,
                "attempts": attempts,
                "failover_count": max(0, attempts - 1),
                "event": "failover" if attempts > 1 else "meta",
            }
            yield f"event: {meta['event']}\n".encode()
            yield ("data: " + json.dumps(meta) + "\n\n").encode()
            try:
                await _inc_requests_total()
                # Streaming hedging decision
                hedging_enabled = bool(getattr(config, "HEDGING_ENABLED", True))
                small_only = bool(getattr(config, "HEDGING_SMALL_MODELS_ONLY", True))
                should_hedge = hedging_enabled and (not small_only or _is_small_model(model_name)) and len(pool) >= 2 and attempts == 1
                if not should_hedge:
                    async for data in attempt_stream(candidate):
                        if not first_byte_recorded:
                            try:
                                await _record_stream_ttfb(model_name, candidate, time.monotonic() - start_time)
                            except Exception:
                                pass
                            first_byte_recorded = True
                        yield data
                    # Success: set sticky mapping
                    await _set_sticky_node(session_id, model_name, candidate)
                    # Record failovers metric for streaming
                    if attempts > 1:
                        try:
                            await redis_client.incrby("lb:failovers_total", attempts - 1)
                            await redis_client.incrby(f"lb:model:{model_name}:failovers_total", attempts - 1)
                        except Exception:
                            pass
                    # Record total stream duration
                    try:
                        await _record_stream_duration(model_name, candidate, time.monotonic() - start_time)
                    except Exception:
                        pass
                    return
                # Hedged path for streaming
                try:
                    p95 = await _estimate_pool_p95(model_name, pool)
                except Exception:
                    p95 = 1.0
                delay_ms = min(int(getattr(config, "HEDGING_MAX_DELAY_MS", 800)), int(1000 * float(getattr(config, "HEDGING_P95_FRACTION", 0.6)) * p95))
                delay_s = max(0, delay_ms) / 1000.0
                # Pick a secondary distinct node
                second_pool = [n for n in pool if n != candidate]
                secondary = await router.select_node(second_pool, model_name, redis_client) if second_pool else None
                if secondary is None:
                    async for data in attempt_stream(candidate):
                        if not first_byte_recorded:
                            try:
                                await _record_stream_ttfb(model_name, candidate, time.monotonic() - start_time)
                            except Exception:
                                pass
                            first_byte_recorded = True
                        yield data
                    await _set_sticky_node(session_id, model_name, candidate)
                    try:
                        await _record_stream_duration(model_name, candidate, time.monotonic() - start_time)
                    except Exception:
                        pass
                    return
                # Zero-delay special case
                primary_gen = attempt_stream(candidate)
                if delay_s <= 0:
                    try:
                        first = await asyncio.wait_for(primary_gen.__anext__(), timeout=0)
                        if not first_byte_recorded:
                            try:
                                await _record_stream_ttfb(model_name, candidate, time.monotonic() - start_time)
                            except Exception:
                                pass
                            first_byte_recorded = True
                        yield first
                        async for data in primary_gen:
                            yield data
                        await _set_sticky_node(session_id, model_name, candidate)
                        try:
                            await _record_stream_duration(model_name, candidate, time.monotonic() - start_time)
                        except Exception:
                            pass
                        return
                    except (asyncio.TimeoutError, StopAsyncIteration, httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError):
                        # Announce hedge start
                        evt = {
                            "request_id": request_id,
                            "model": model_name,
                            "primary": candidate,
                            "secondary": secondary,
                            "event": "hedge_start",
                        }
                        yield b"event: hedge_start\n"
                        yield ("data: " + json.dumps(evt) + "\n\n").encode()
                        try:
                            await redis_client.incrby("lb:hedges_total", 1)
                        except Exception:
                            pass
                        sgen = attempt_stream(secondary)
                        first = await sgen.__anext__()
                        if not first_byte_recorded:
                            try:
                                await _record_stream_ttfb(model_name, secondary, time.monotonic() - start_time)
                            except Exception:
                                pass
                            first_byte_recorded = True
                        yield first
                        async for data in sgen:
                            yield data
                        await _set_sticky_node(session_id, model_name, secondary)
                        try:
                            await redis_client.incrby(f"lb:hedge_wins:{model_name}|{secondary}", 1)
                            await redis_client.sadd("lb:hedge_wins_models", model_name)
                            await redis_client.incrby(f"lb:hedge_wins_model:{model_name}", 1)
                        except Exception:
                            pass
                        # Announce hedge winner
                        w_evt = {
                            "request_id": request_id,
                            "model": model_name,
                            "primary": candidate,
                            "secondary": secondary,
                            "winner": secondary,
                            "event": "hedge_winner",
                        }
                        yield b"event: hedge_winner\n"
                        yield ("data: " + json.dumps(w_evt) + "\n\n").encode()
                        try:
                            await _record_stream_duration(model_name, secondary, time.monotonic() - start_time)
                        except Exception:
                            pass
                        return
                # Delayed hedge: wait for first chunk from primary within delay, else race
                try:
                    first = await asyncio.wait_for(attempt_stream(candidate).__anext__(), timeout=delay_s)
                    if not first_byte_recorded:
                        try:
                            await _record_stream_ttfb(model_name, candidate, time.monotonic() - start_time)
                        except Exception:
                            pass
                        first_byte_recorded = True
                    yield first
                    async for data in attempt_stream(candidate):
                        yield data
                    await _set_sticky_node(session_id, model_name, candidate)
                    try:
                        await _record_stream_duration(model_name, candidate, time.monotonic() - start_time)
                    except Exception:
                        pass
                    return
                except asyncio.TimeoutError:
                    # Announce hedge start
                    evt = {
                        "request_id": request_id,
                        "model": model_name,
                        "primary": candidate,
                        "secondary": secondary,
                        "event": "hedge_start",
                    }
                    yield b"event: hedge_start\n"
                    yield ("data: " + json.dumps(evt) + "\n\n").encode()
                    try:
                        await redis_client.incrby("lb:hedges_total", 1)
                    except Exception:
                        pass
                    pgen = attempt_stream(candidate)
                    sgen = attempt_stream(secondary)
                    pfirst = asyncio.create_task(pgen.__anext__())
                    sfirst = asyncio.create_task(sgen.__anext__())
                    done, pending = await asyncio.wait({pfirst, sfirst}, return_when=asyncio.FIRST_COMPLETED)
                    dt = next(iter(done))
                    winner = candidate if dt is pfirst else secondary
                    wgen = pgen if dt is pfirst else sgen
                    # Cancel the other pending task
                    for t in pending:
                        t.cancel()
                    try:
                        first = dt.result()
                    except Exception as e:
                        # try the other if failed
                        try:
                            other = next(iter(pending))
                            first = await other
                            winner = secondary if winner == candidate else candidate
                            wgen = sgen if wgen is pgen else pgen
                        except Exception:
                            raise e
                    if not first_byte_recorded:
                        try:
                            await _record_stream_ttfb(model_name, winner, time.monotonic() - start_time)
                        except Exception:
                            pass
                        first_byte_recorded = True
                    yield first
                    async for data in wgen:
                        yield data
                    await _set_sticky_node(session_id, model_name, winner)
                    try:
                        await redis_client.incrby(f"lb:hedge_wins:{model_name}|{winner}", 1)
                    except Exception:
                        pass
                    # Announce hedge winner
                    w_evt = {
                        "request_id": request_id,
                        "model": model_name,
                        "primary": candidate,
                        "secondary": secondary,
                        "winner": winner,
                        "event": "hedge_winner",
                    }
                    yield b"event: hedge_winner\n"
                    yield ("data: " + json.dumps(w_evt) + "\n\n").encode()
                    try:
                        await _record_stream_duration(model_name, winner, time.monotonic() - start_time)
                    except Exception:
                        pass
                    return
            except (httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError) as e:
                logger.warning("[req=%s] Upstream error from %s: %s (attempt %d/%d)", request_id, candidate, e, attempts, budget)
                await _record_failure(candidate)
                last_error = e
                if not isinstance(e, CapacityError):
                    all_capacity = False
                continue
        # If all attempts failed, return a final error message chunk
        err_msg = {
            "error": {
                "message": "All upstream nodes failed for model.",
                "model": model_name,
                "attempts": attempts,
                "nodes": tried_order
            }
        }
        if last_error:
            logger.error("All upstream attempts failed: %s", last_error)
        # For streaming, we cannot change status mid-stream; yield error body
        yield json.dumps(err_msg).encode()

    if is_stream:
        # Streaming behavior (existing)
        if forced_node:
            if forced_node not in eligible_nodes:
                raise HTTPException(status_code=404, detail=f"Requested node '{forced_node}' is not eligible for model '{model_name}'.")
            async def stream_single():
                try:
                    await _inc_requests_total()
                    async for data in attempt_stream(forced_node):
                        yield data
                    await _set_sticky_node(session_id, model_name, forced_node)
                except (httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError) as e:
                    err = {"error": {"message": f"Upstream error from {forced_node}: {e}"}}
                    yield json.dumps(err).encode()
            return StreamingResponse(stream_single(), media_type="text/event-stream", headers={
                "x-selected-model": model_name,
                "x-routed-node": forced_node,
                "x-request-id": request_id,
                "x-on-demand-wait": "true" if on_demand_wait else "false",
                "x-warm-wait-ms": str(warm_wait_ms),
                "x-capacity-state": "ok",
                "x-model-defaulted": "true" if model_defaulted else "false",
            })
        else:
            # Pre-check: if all nodes saturated (inflight >= maxconn), respond 429 immediately
            try:
                healthy = eligible_nodes
                saturated = True
                model_sat = False
                try:
                    m_inflight = await redis_client.get(f"model:{model_name}:inflight")
                    m_max = await redis_client.get(f"model:{model_name}:maxconn")
                    m_inflight = int(m_inflight) if m_inflight else 0
                    m_max = int(m_max) if m_max not in (None, "", "0") else None
                    if m_max is not None and m_inflight >= m_max:
                        model_sat = True
                except Exception:
                    pass
                for n in healthy:
                    inflight_val = await redis_client.get(f"node:{n}:inflight")
                    maxconn_val = await redis_client.get(f"node:{n}:maxconn")
                    inflight = int(inflight_val) if inflight_val is not None else 0
                    maxconn = int(maxconn_val) if maxconn_val not in (None, "", "0") else None
                    # If no maxconn configured, treat as not saturated
                    if maxconn is None or inflight < maxconn:
                        saturated = False
                        break
                if saturated:
                    raise HTTPException(status_code=429, detail="All nodes are at capacity for this model.", headers={"Retry-After": str(config.RETRY_AFTER_SECS), "x-request-id": request_id, "x-capacity-state": ("model_saturated" if model_sat else "cluster_saturated")})
            except HTTPException:
                raise
            except Exception:
                pass
            # Compute initial routed node for header (best-effort)
            try:
                remaining = list(eligible_nodes)
                closed = []
                for n in remaining:
                    if not await _is_circuit_open(n):
                        closed.append(n)
                pool = closed if closed else remaining
                first = None
                if not forced_node:
                    sticky = await _get_sticky_node(session_id, model_name)
                    if sticky and sticky in pool:
                        first = sticky
                if first is None:
                    first = await router.select_node(pool, model_name, redis_client)
            except Exception:
                first = None
            return StreamingResponse(stream_with_failover(eligible_nodes), media_type="text/event-stream", headers={
                "x-selected-model": model_name,
                "x-routed-node": first or "",
                "x-request-id": request_id,
                "x-on-demand-wait": "true" if on_demand_wait else "false",
                "x-warm-wait-ms": str(warm_wait_ms),
                "x-capacity-state": "ok",
                "x-model-defaulted": "true" if model_defaulted else "false",
            })

    # Non-streaming behavior: aggregate JSON and return a Response
    async def attempt_request(node: str):
        # Determine URL and headers based on backend type (cloud vs local)
        if _is_cloud_backend(node):
            url = _get_cloud_url(node, "/chat/completions")
            if not url:
                raise httpx.RequestError(f"Cloud backend URL not configured for {node}")
            req_headers = _get_cloud_headers(node, headers)
        else:
            url = f"http://{node}/v1/chat/completions"
            req_headers = headers

        logger.info("[req=%s] Routing (non-stream) request for model '%s' to %s", request_id, model_name, node)
        acquired = await _acquire_slot(node)
        if not acquired:
            logger.info("[req=%s] Node %s at capacity; skipping", request_id, node)
            raise CapacityError("node")
        model_ok = await _acquire_model_slot(model_name)
        if not model_ok:
            await _dec_inflight(node)
            raise CapacityError("model")
        try:
            try:
                resp = await http_client.post(url, json=body, headers=req_headers)
                status_code = resp.status_code
                content = resp.content
                ctype = resp.headers.get("content-type", "application/json")
            except AttributeError:
                # Compatibility for test fakes that implement only `.stream(...)`
                async with http_client.stream("POST", url, json=body, headers=req_headers) as response:
                    status_code = getattr(response, "status_code", 200)
                    # Handle rate limiting (429)
                    if status_code == 429:
                        retry_after = getattr(response, "headers", {}).get("retry-after")
                        retry_after_secs = float(retry_after) if retry_after else None
                        await _record_rate_limit(node, retry_after_secs)
                        raise RateLimitError(node, retry_after_secs)
                    # Treat 5xx and 404 as retryable across nodes (LM Studio variants sometimes 404 chat endpoint)
                    if status_code and (status_code >= 500 or status_code == 404):
                        await _record_failure(node)
                        raise httpx.HTTPStatusError("Upstream retryable error", request=None, response=None)
                    chunks = bytearray()
                    async for chunk in response.aiter_bytes():
                        chunks.extend(chunk)
                    content = bytes(chunks)
                    ctype = "application/json"
            # Handle rate limiting (429)
            if status_code == 429:
                retry_after = resp.headers.get("retry-after")
                retry_after_secs = float(retry_after) if retry_after else None
                await _record_rate_limit(node, retry_after_secs)
                raise RateLimitError(node, retry_after_secs)
            # Evaluate retry conditions for normal client path
            if status_code and (status_code >= 500 or status_code == 404):
                await _record_failure(node)
                raise httpx.HTTPStatusError("Upstream retryable error", request=None, response=None)
            await _record_success(node)
            await _clear_rate_limit(node)
            out = Response(content=content, media_type=ctype, status_code=status_code)
            out.headers["x-selected-model"] = model_name
            out.headers["x-routed-node"] = node
            out.headers["x-request-id"] = request_id
            out.headers["x-on-demand-wait"] = "true" if on_demand_wait else "false"
            out.headers["x-warm-wait-ms"] = str(warm_wait_ms)
            out.headers["x-capacity-state"] = "ok"
            out.headers["x-model-defaulted"] = "true" if model_defaulted else "false"
            # Success: set sticky mapping
            await _set_sticky_node(session_id, model_name, node)
            return out
        finally:
            await _dec_inflight(node)
            await _dec_model(model_name)

    if forced_node:
        if forced_node not in eligible_nodes:
            raise HTTPException(status_code=404, detail=f"Requested node '{forced_node}' is not eligible for model '{model_name}'.")
        try:
            await _inc_requests_total()
            return await attempt_request(forced_node)
        except (httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError) as e:
            raise HTTPException(status_code=502, detail=f"Upstream error from {forced_node}: {e}")

    # Failover loop (non-stream)
    attempts = 0
    tried = set()
    last_error = None
    budget = min(len(eligible_nodes), 1 + config.MAX_RETRIES)
    all_capacity = True
    tried_order = []
    while attempts < budget:
        remaining = [n for n in eligible_nodes if n not in tried]
        closed = []
        for n in remaining:
            if not await _is_circuit_open(n):
                closed.append(n)
        pool = closed if closed else remaining
        candidate = None
        if attempts == 0 and not forced_node:
            sticky = await _get_sticky_node(session_id, model_name)
            if sticky and sticky in pool:
                candidate = sticky
        if candidate is None:
            candidate = await router.select_node(pool, model_name, redis_client)
        if candidate is None:
            break
        tried.add(candidate)
        tried_order.append(candidate)
        attempts += 1

        # Hedging: optional duplicate attempt after delay for small models
        hedging_enabled = bool(getattr(config, "HEDGING_ENABLED", True))
        small_only = bool(getattr(config, "HEDGING_SMALL_MODELS_ONLY", True))
        should_hedge = hedging_enabled and (not small_only or _is_small_model(model_name)) and len(pool) >= 2 and attempts == 1

        if should_hedge:
            # Select a distinct secondary candidate
            second_pool = [n for n in pool if n != candidate]
            secondary = await router.select_node(second_pool, model_name, redis_client) if second_pool else None
            if secondary is not None:
                # Estimate delay
                p95 = await _estimate_pool_p95(model_name, pool)
                delay_ms = min(int(getattr(config, "HEDGING_MAX_DELAY_MS", 800)), int(1000 * float(getattr(config, "HEDGING_P95_FRACTION", 0.6)) * p95))
                delay_s = max(0, delay_ms) / 1000.0
                # Special case: zero delay -> try primary, and on immediate failure hedge to secondary synchronously
                if delay_s <= 0:
                    try:
                        await _inc_requests_total()
                        resp = await attempt_request(candidate)
                        resp.headers["x-attempts"] = str(attempts)
                        resp.headers["x-failover-count"] = str(max(0, attempts - 1))
                        resp.headers["x-retry-count"] = str(max(0, attempts - 1))
                        resp.headers["x-hedged"] = "false"
                        resp.headers["x-hedge-winner"] = ""
                        try:
                            await _record_latency(model_name, candidate, time.monotonic() - start_time)
                        except Exception:
                            pass
                        return resp
                    except (httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError):
                        # Immediate hedge to secondary
                        try:
                            await redis_client.incrby("lb:hedges_total", 1)
                        except Exception:
                            pass
                        await _inc_requests_total()
                        resp = await attempt_request(secondary)
                        resp.headers["x-attempts"] = str(attempts)
                        resp.headers["x-failover-count"] = str(max(0, attempts - 1))
                        resp.headers["x-retry-count"] = str(max(0, attempts - 1))
                        resp.headers["x-hedged"] = "true"
                        resp.headers["x-hedge-winner"] = secondary
                        try:
                            await redis_client.incrby(f"lb:hedge_wins:{model_name}|{secondary}", 1)
                            await redis_client.sadd("lb:hedge_wins_models", model_name)
                            await redis_client.incrby(f"lb:hedge_wins_model:{model_name}", 1)
                        except Exception:
                            pass
                        try:
                            await _record_latency(model_name, secondary, time.monotonic() - start_time)
                        except Exception:
                            pass
                        return resp

                # Launch primary task
                await _inc_requests_total()
                t1 = asyncio.create_task(attempt_request(candidate))
                winner_node = None
                try:
                    # Give primary a head start equal to delay
                    try:
                        resp = await asyncio.wait_for(t1, timeout=delay_s)
                        # Primary finished within delay; no hedging launched
                        resp.headers["x-attempts"] = str(attempts)
                        resp.headers["x-failover-count"] = str(max(0, attempts - 1))
                        resp.headers["x-retry-count"] = str(max(0, attempts - 1))
                        resp.headers["x-hedged"] = "false"
                        resp.headers["x-hedge-winner"] = ""
                        try:
                            await _record_latency(model_name, candidate, time.monotonic() - start_time)
                        except Exception:
                            pass
                        return resp
                    except asyncio.TimeoutError:
                        # Launch hedge
                        try:
                            await redis_client.incrby("lb:hedges_total", 1)
                        except Exception:
                            pass
                        t2 = asyncio.create_task(attempt_request(secondary))
                        # Wait for first to complete
                        done, pending = await asyncio.wait({t1, t2}, return_when=asyncio.FIRST_COMPLETED)
                        d = next(iter(done))
                        try:
                            resp = d.result()
                            winner_node = candidate if d is t1 else secondary
                            # Cancel the other
                            for p in pending:
                                p.cancel()
                            # Ensure exceptions/cancellations are collected
                            try:
                                await asyncio.gather(*([t1, t2]), return_exceptions=True)
                            except Exception:
                                pass
                            # Mark winner metrics
                            try:
                                await redis_client.incrby(f"lb:hedge_wins:{model_name}|{winner_node}", 1)
                                await redis_client.sadd("lb:hedge_wins_models", model_name)
                                await redis_client.incrby(f"lb:hedge_wins_model:{model_name}", 1)
                            except Exception:
                                pass
                            # Headers and latency
                            resp.headers["x-attempts"] = str(attempts)
                            resp.headers["x-failover-count"] = str(max(0, attempts - 1))
                            resp.headers["x-retry-count"] = str(max(0, attempts - 1))
                            resp.headers["x-hedged"] = "true"
                            resp.headers["x-hedge-winner"] = winner_node or ""
                            try:
                                await _record_latency(model_name, winner_node or candidate, time.monotonic() - start_time)
                            except Exception:
                                pass
                            return resp
                        except Exception as e:
                            # First completed with error; wait for the other
                            other = next(iter(pending)) if pending else None
                            if other is not None:
                                try:
                                    resp = await other
                                    winner_node = secondary if other is t2 else candidate
                                    resp.headers["x-attempts"] = str(attempts)
                                    resp.headers["x-failover-count"] = str(max(0, attempts - 1))
                                    resp.headers["x-retry-count"] = str(max(0, attempts - 1))
                                    resp.headers["x-hedged"] = "true"
                                    resp.headers["x-hedge-winner"] = winner_node or ""
                                    try:
                                        await redis_client.incrby(f"lb:hedge_wins:{model_name}|{winner_node}", 1)
                                        await redis_client.sadd("lb:hedge_wins_models", model_name)
                                        await redis_client.incrby(f"lb:hedge_wins_model:{model_name}", 1)
                                    except Exception:
                                        pass
                                    try:
                                        await _record_latency(model_name, winner_node or candidate, time.monotonic() - start_time)
                                    except Exception:
                                        pass
                                    # Ensure exceptions/cancellations are collected
                                    try:
                                        await asyncio.gather(*([t1, t2]), return_exceptions=True)
                                    except Exception:
                                        pass
                                    return resp
                                except Exception as e2:
                                    # Both failed; fall through to normal failure handling
                                    await _record_failure(candidate)
                                    await _record_failure(secondary)
                                    last_error = e2
                                    all_capacity = False
                                    tried.add(secondary)
                                    tried_order.append(secondary)
                                    continue
                except (httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError) as e:
                    # Primary threw before delay elapsed; treat as normal failure (no hedge launched)
                    await _record_failure(candidate)
                    last_error = e
                    if not isinstance(e, CapacityError):
                        all_capacity = False
                    continue

        # No hedging path or not applicable: normal single attempt
        try:
            await _inc_requests_total()
            resp = await attempt_request(candidate)
            resp.headers["x-attempts"] = str(attempts)
            resp.headers["x-failover-count"] = str(max(0, attempts - 1))
            resp.headers["x-retry-count"] = str(max(0, attempts - 1))
            resp.headers["x-hedged"] = "false"
            resp.headers["x-hedge-winner"] = ""
            try:
                await _record_latency(model_name, candidate, time.monotonic() - start_time)
            except Exception:
                pass
            return resp
        except (httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError) as e:
            logger.warning("[req=%s] Chat (non-stream) upstream error from %s: %s (attempt %d/%d)", request_id, candidate, e, attempts, budget)
            await _record_failure(candidate)
            last_error = e
            if not isinstance(e, CapacityError):
                all_capacity = False
            continue
    if all_capacity:
        # Capacity exhausted for this model; attempt cross-model fallback if enabled
        fb = await _find_fallback_model(model_name)
        if fb:
            logger.warning("Capacity exhausted for model '%s'; attempting fallback '%s'", model_name, fb)
            body["model"] = fb
            # Re-evaluate eligible nodes for fallback model
            fb_nodes = await get_eligible_nodes(fb)
            if not fb_nodes:
                # No fallback capacity after all
                raise HTTPException(status_code=429, detail={"message": "All nodes are at capacity for this model.", "model": model_name, "attempts": attempts, "nodes": tried_order}, headers={"Retry-After": str(config.RETRY_AFTER_SECS), "x-request-id": request_id, "x-attempts": str(attempts), "x-failover-count": str(max(0, attempts-1)), "x-capacity-state": "cluster_saturated"})
            # Try a single attempt on fallback pool using same logic but without recursion complexity
            candidate = await router.select_node(fb_nodes, fb, redis_client)
            if candidate is None:
                raise HTTPException(status_code=429, detail={"message": "No eligible nodes for fallback model.", "model": fb}, headers={"Retry-After": str(config.RETRY_AFTER_SECS), "x-request-id": request_id})
            await _inc_requests_total()
            resp = await attempt_request(candidate)
            resp.headers["x-selected-model"] = fb
            resp.headers["x-fallback-model"] = fb
            resp.headers["x-retry-count"] = str(max(0, attempts - 1))
            return resp
        # No fallback configured or available
        raise HTTPException(status_code=429, detail={"message": "All nodes are at capacity for this model.", "model": model_name, "attempts": attempts, "nodes": tried_order}, headers={"Retry-After": str(config.RETRY_AFTER_SECS), "x-request-id": request_id, "x-attempts": str(attempts), "x-failover-count": str(max(0, attempts-1)), "x-capacity-state": "cluster_saturated"})
    # Failures for the model; attempt fallback if configured
    fb = await _find_fallback_model(model_name)
    if fb:
        logger.warning("All upstream nodes failed for model '%s'; attempting fallback '%s'", model_name, fb)
        body["model"] = fb
        fb_nodes = await get_eligible_nodes(fb)
        if fb_nodes:
            candidate = await router.select_node(fb_nodes, fb, redis_client)
            if candidate is not None:
                await _inc_requests_total()
                resp = await attempt_request(candidate)
                resp.headers["x-selected-model"] = fb
                resp.headers["x-fallback-model"] = fb
                resp.headers["x-retry-count"] = str(max(0, attempts - 1))
                return resp
    raise HTTPException(status_code=502, detail={"message": "All upstream nodes failed for chat model.", "model": model_name, "attempts": attempts, "nodes": tried_order}, headers={"x-request-id": request_id, "x-attempts": str(attempts), "x-failover-count": str(max(0, attempts-1))})

@app.api_route("/v1/embeddings", methods=["POST"])
async def embeddings(request: Request):
    try:
        raw_body = await request.json()
    except json.JSONDecodeError as exc:
        logger.warning("Invalid JSON payload for /v1/embeddings: %s", exc)
        raise HTTPException(status_code=400, detail="Request body must be valid JSON.")
    except Exception as exc:
        logger.warning("Failed to parse JSON payload for /v1/embeddings: %s", exc)
        raise HTTPException(status_code=400, detail="Request body must be valid JSON.")

    sanitized = sanitize_embeddings_request(raw_body)
    if sanitized.default_applied:
        logger.info(
            "Embeddings request missing model; applied default '%s'", sanitized.default_applied
        )
    if sanitized.missing_fields:
        raise HTTPException(
            status_code=400,
            detail={
                "message": "Missing required fields.",
                "missing": sanitized.missing_fields,
            },
        )

    body = sanitized.payload
    model_name = sanitized.model_name
    model_defaulted = sanitized.default_applied is not None

    if _is_model_sentinel(model_name):
        prefer_intersection = request.query_params.get("require_all", "false").lower() in ("1", "true", "yes")
        resolved = await _resolve_auto_model(prefer_intersection=prefer_intersection)
        if not resolved:
            raise HTTPException(status_code=404, detail="No models available for auto selection.")
        body["model"] = resolved
        model_name = resolved
    eligible_nodes = await get_eligible_nodes_for_model(model_name)
    on_demand_wait = False
    warm_wait_ms = 0
    if not eligible_nodes:
        try:
            t0_warm = time.monotonic()
            waited = await _on_demand_wait_for_model(model_name)
            warm_wait_ms = int((time.monotonic() - t0_warm) * 1000)
            if waited:
                eligible_nodes = waited
                on_demand_wait = True
        except Exception:
            pass
    if not eligible_nodes:
        raise HTTPException(status_code=404, detail=f"No healthy nodes found for model '{model_name}'.")

    headers = {key: value for key, value in request.headers.items() if key.lower() not in ('host', 'content-length')}
    request_id = request.headers.get("x-request-id") or str(uuid.uuid4())
    headers["x-request-id"] = request_id
    session_id = request.headers.get("x-session-id")

    async def attempt_request(node: str):
        # Determine URL and headers based on backend type (cloud vs local)
        if _is_cloud_backend(node):
            url = _get_cloud_url(node, "/embeddings")
            if not url:
                raise httpx.RequestError(f"Cloud backend URL not configured for {node}")
            req_headers = _get_cloud_headers(node, headers)
        else:
            url = f"http://{node}/v1/embeddings"
            req_headers = headers

        logger.info("[req=%s] Routing embeddings request for model '%s' to %s", request_id, model_name, node)
        acquired = await _acquire_slot(node)
        if not acquired:
            logger.info("[req=%s] Node %s at capacity; skipping", request_id, node)
            raise CapacityError("node")
        model_ok = await _acquire_model_slot(model_name)
        if not model_ok:
            await _dec_inflight(node)
            raise CapacityError("model")
        try:
            try:
                resp = await http_client.post(url, json=body, headers=req_headers)
                status_code = resp.status_code
                content = resp.content
                ctype = resp.headers.get("content-type", "application/json")
            except AttributeError:
                # Compatibility path for fakes
                async with http_client.stream("POST", url, json=body, headers=req_headers) as response:
                    status_code = getattr(response, "status_code", 200)
                    # Handle rate limiting (429)
                    if status_code == 429:
                        retry_after = getattr(response, "headers", {}).get("retry-after")
                        retry_after_secs = float(retry_after) if retry_after else None
                        await _record_rate_limit(node, retry_after_secs)
                        raise RateLimitError(node, retry_after_secs)
                    if status_code and status_code >= 500:
                        await _record_failure(node)
                        raise httpx.HTTPStatusError("Upstream 5xx", request=None, response=None)
                    chunks = bytearray()
                    async for chunk in response.aiter_bytes():
                        chunks.extend(chunk)
                    content = bytes(chunks)
                    ctype = "application/json"
            # Handle rate limiting (429)
            if status_code == 429:
                retry_after = resp.headers.get("retry-after")
                retry_after_secs = float(retry_after) if retry_after else None
                await _record_rate_limit(node, retry_after_secs)
                raise RateLimitError(node, retry_after_secs)
            if status_code and status_code >= 500:
                await _record_failure(node)
                raise httpx.HTTPStatusError("Upstream 5xx", request=None, response=None)
            await _record_success(node)
            await _clear_rate_limit(node)
            out = Response(content=content, media_type=ctype, status_code=status_code)
            out.headers["x-selected-model"] = model_name
            out.headers["x-routed-node"] = node
            out.headers["x-request-id"] = request_id
            out.headers["x-on-demand-wait"] = "true" if on_demand_wait else "false"
            out.headers["x-warm-wait-ms"] = str(warm_wait_ms)
            out.headers["x-capacity-state"] = "ok"
            out.headers["x-model-defaulted"] = "true" if model_defaulted else "false"
            await _set_sticky_node(session_id, model_name, node)
            return out
        finally:
            await _dec_inflight(node)
            await _dec_model(model_name)

    attempts = 0
    tried = set()
    last_error = None
    all_capacity = True
    budget = min(len(eligible_nodes), 1 + config.MAX_RETRIES)
    tried_order = []
    while attempts < budget:
        remaining = [n for n in eligible_nodes if n not in tried]
        closed = []
        for n in remaining:
            if not await _is_circuit_open(n):
                closed.append(n)
        pool = closed if closed else remaining
        candidate = None
        if attempts == 0:
            sticky = await _get_sticky_node(session_id, model_name)
            if sticky and sticky in pool:
                candidate = sticky
        if candidate is None:
            candidate = await router.select_node(pool, model_name, redis_client)
        if candidate is None:
            break
        tried.add(candidate)
        tried_order.append(candidate)
        attempts += 1
        try:
            await _inc_requests_total()
            resp = await attempt_request(candidate)
            # On success, add attempt headers and record metrics
            resp.headers["x-attempts"] = str(attempts)
            resp.headers["x-failover-count"] = str(max(0, attempts - 1))
            resp.headers["x-retry-count"] = str(max(0, attempts - 1))
            resp.headers["x-hedged"] = "false"
            resp.headers["x-hedge-winner"] = ""
            # Failover counters
            if attempts > 1:
                try:
                    await redis_client.incrby("lb:failovers_total", attempts - 1)
                    await redis_client.incrby(f"lb:model:{model_name}:failovers_total", attempts - 1)
                except Exception:
                    pass
            # Latency histogram (non-stream)
            try:
                # For embeddings, we measure end-to-end as well
                elapsed = time.monotonic() - start_time
                await _record_latency(model_name, candidate, elapsed)
            except Exception:
                pass
            return resp
        except (httpx.RequestError, httpx.HTTPStatusError, CapacityError, RateLimitError) as e:
            logger.warning("[req=%s] Embeddings upstream error from %s: %s (attempt %d/%d)", request_id, candidate, e, attempts, budget)
            await _record_failure(candidate)
            last_error = e
            if not isinstance(e, CapacityError):
                all_capacity = False
            continue
    if all_capacity:
        raise HTTPException(status_code=429, detail={"message": "All nodes are at capacity for this model.", "model": model_name, "attempts": attempts, "nodes": tried_order}, headers={"Retry-After": str(config.RETRY_AFTER_SECS), "x-request-id": request_id, "x-attempts": str(attempts), "x-failover-count": str(max(0, attempts-1))})
    raise HTTPException(status_code=502, detail={"message": "All upstream nodes failed for embeddings model.", "model": model_name, "attempts": attempts, "nodes": tried_order}, headers={"x-request-id": request_id, "x-attempts": str(attempts), "x-failover-count": str(max(0, attempts-1))})

@app.get("/health", status_code=200)
async def health_check(response: Response):
    """Checks the health of the load balancer and its node cluster."""
    healthy_node_count = await redis_client.scard("nodes:healthy")
    
    if healthy_node_count >= config.MIN_HEALTHY_NODES:
        return {
            "status": "healthy",
            "nodes_found": healthy_node_count,
            "minimum_required": config.MIN_HEALTHY_NODES
        }
    else:
        response.status_code = 503
        return {
            "status": "unhealthy",
            "nodes_found": healthy_node_count,
            "minimum_required": config.MIN_HEALTHY_NODES
        }

@app.get("/metrics")
async def metrics():
    """Prometheus-style metrics exposition without external deps."""
    lines = []
    lines.append("# HELP ai_lb_requests_total Total requests handled by the LB")
    lines.append("# TYPE ai_lb_requests_total counter")
    total = await redis_client.get("lb:requests_total")
    lines.append(f"ai_lb_requests_total {int(total) if total else 0}")

    healthy = await redis_client.smembers("nodes:healthy")
    lines.append("# HELP ai_lb_up Up status of nodes (1 healthy, 0 otherwise)")
    lines.append("# TYPE ai_lb_up gauge")
    for n in healthy:
        lines.append(f'ai_lb_up{{node="{n}"}} 1')

    lines.append("# HELP ai_lb_inflight Current in-flight requests per node")
    lines.append("# TYPE ai_lb_inflight gauge")
    for n in healthy:
        inflight = await redis_client.get(f"node:{n}:inflight")
        lines.append(f'ai_lb_inflight{{node="{n}"}} {int(inflight) if inflight else 0}')

    lines.append("# HELP ai_lb_failures Recent failure count per node")
    lines.append("# TYPE ai_lb_failures gauge")
    for n in healthy:
        failures = await redis_client.get(f"node:{n}:failures")
        lines.append(f'ai_lb_failures{{node="{n}"}} {int(failures) if failures else 0}')

    # Failovers total (overall and per model)
    lines.append("# HELP ai_lb_failovers_total Total failovers across all requests")
    lines.append("# TYPE ai_lb_failovers_total counter")
    total_failovers = await redis_client.get("lb:failovers_total")
    lines.append(f"ai_lb_failovers_total {int(total_failovers) if total_failovers else 0}")
    # Per-model failovers
    # We don't track model list centrally; infer from keys stored
    # by scanning models in latency series and model failover keys indirectly.
    # Keep simple: scan latency series' models
    series = await redis_client.smembers("lb:latency_series")
    models = set()
    for s in series:
        try:
            m, _ = s.split("|", 1)
            models.add(m)
        except Exception:
            continue
    for m in models:
        mf = await redis_client.get(f"lb:model:{m}:failovers_total")
        if mf:
            lines.append(f'ai_lb_failovers_total{{model="{m}"}} {int(mf)}')

    # Rate limit metrics
    lines.append("# HELP ai_lb_rate_limits_total Total rate limit (429) responses")
    lines.append("# TYPE ai_lb_rate_limits_total counter")
    rate_limits_total = await redis_client.get("lb:rate_limits_total")
    lines.append(f"ai_lb_rate_limits_total {int(rate_limits_total) if rate_limits_total else 0}")

    # Per-node rate limits
    lines.append("# HELP ai_lb_rate_limits Rate limit counts per node")
    lines.append("# TYPE ai_lb_rate_limits counter")
    for n in healthy:
        rl = await redis_client.get(f"lb:rate_limits:{n}")
        if rl:
            lines.append(f'ai_lb_rate_limits{{node="{n}"}} {int(rl)}')

    # Hedging metrics
    lines.append("# HELP ai_lb_hedges_total Total hedged duplicate attempts")
    lines.append("# TYPE ai_lb_hedges_total counter")
    hedges_total = await redis_client.get("lb:hedges_total")
    lines.append(f"ai_lb_hedges_total {int(hedges_total) if hedges_total else 0}")

    lines.append("# HELP ai_lb_hedge_wins Hedge wins per model/node")
    lines.append("# TYPE ai_lb_hedge_wins counter")
    # Infer winners by scanning known latency series for keys
    if series:
        for s in series:
            try:
                m, n = s.split("|", 1)
            except Exception:
                continue
            w = await redis_client.get(f"lb:hedge_wins:{s}")
            if w:
                lines.append(f'ai_lb_hedge_wins{{model="{m}",node="{n}"}} {int(w)}')

    # Latency histogram per model and node
    lines.append("# HELP ai_lb_latency_seconds Request latency histogram per model/node")
    lines.append("# TYPE ai_lb_latency_seconds histogram")
    # Iterate known series
    for s in series:
        try:
            m, n = s.split("|", 1)
        except Exception:
            continue
        cumulative = 0
        for le in _LAT_BUCKETS:
            val = await redis_client.get(f"lb:latency_bucket:{s}:{le}")
            v = int(val) if val else 0
            cumulative = v  # already stored as cumulative
            le_str = "+Inf" if le == float("inf") else ("%.2f" % le).rstrip('0').rstrip('.')
            lines.append(f'ai_lb_latency_seconds_bucket{{model="{m}",node="{n}",le="{le_str}"}} {cumulative}')
        s_sum = await redis_client.get(f"lb:latency_sum:{s}")
        s_cnt = await redis_client.get(f"lb:latency_count:{s}")
        lines.append(f'ai_lb_latency_seconds_sum{{model="{m}",node="{n}"}} {float(s_sum) if s_sum else 0.0}')
        lines.append(f'ai_lb_latency_seconds_count{{model="{m}",node="{n}"}} {int(s_cnt) if s_cnt else 0}')

    # Stream TTFB histogram
    ttfb_series = await redis_client.smembers("lb:stream_ttfb_series")
    if ttfb_series:
        lines.append("# HELP ai_lb_stream_ttfb_seconds Time-to-first-byte for streaming requests")
        lines.append("# TYPE ai_lb_stream_ttfb_seconds histogram")
        for s in ttfb_series:
            try:
                m, n = s.split("|", 1)
            except Exception:
                continue
            for le in _LAT_BUCKETS:
                val = await redis_client.get(f"lb:stream_ttfb_bucket:{s}:{le}")
                v = int(val) if val else 0
                le_str = "+Inf" if le == float("inf") else ("%.2f" % le).rstrip('0').rstrip('.')
                lines.append(f'ai_lb_stream_ttfb_seconds_bucket{{model="{m}",node="{n}",le="{le_str}"}} {v}')
            s_sum = await redis_client.get(f"lb:stream_ttfb_sum:{s}")
            s_cnt = await redis_client.get(f"lb:stream_ttfb_count:{s}")
            lines.append(f'ai_lb_stream_ttfb_seconds_sum{{model="{m}",node="{n}"}} {float(s_sum) if s_sum else 0.0}')
            lines.append(f'ai_lb_stream_ttfb_seconds_count{{model="{m}",node="{n}"}} {int(s_cnt) if s_cnt else 0}')

    # Stream duration histogram
    dur_series = await redis_client.smembers("lb:stream_duration_series")
    if dur_series:
        lines.append("# HELP ai_lb_stream_duration_seconds Total stream duration for streaming requests")
        lines.append("# TYPE ai_lb_stream_duration_seconds histogram")
        for s in dur_series:
            try:
                m, n = s.split("|", 1)
            except Exception:
                continue
            for le in _LAT_BUCKETS:
                val = await redis_client.get(f"lb:stream_duration_bucket:{s}:{le}")
                v = int(val) if val else 0
                le_str = "+Inf" if le == float("inf") else ("%.2f" % le).rstrip('0').rstrip('.')
                lines.append(f'ai_lb_stream_duration_seconds_bucket{{model="{m}",node="{n}",le="{le_str}"}} {v}')
            s_sum = await redis_client.get(f"lb:stream_duration_sum:{s}")
            s_cnt = await redis_client.get(f"lb:stream_duration_count:{s}")
            lines.append(f'ai_lb_stream_duration_seconds_sum{{model="{m}",node="{n}"}} {float(s_sum) if s_sum else 0.0}')
            lines.append(f'ai_lb_stream_duration_seconds_count{{model="{m}",node="{n}"}} {int(s_cnt) if s_cnt else 0}')

    # Aggregate hedge wins per model
    try:
        models = await redis_client.smembers("lb:hedge_wins_models")
        lines.append("# HELP ai_lb_hedge_wins_total Hedge wins per model")
        lines.append("# TYPE ai_lb_hedge_wins_total counter")
        for m in models:
            val = await redis_client.get(f"lb:hedge_wins_model:{m}")
            v = int(val) if val else 0
            lines.append(f'ai_lb_hedge_wins_total{{model="{m}"}} {v}')
    except Exception:
        pass

    # Multi-backend execution metrics
    lines.append("# HELP ai_lb_multi_exec_total Total multi-backend execution requests by mode")
    lines.append("# TYPE ai_lb_multi_exec_total counter")
    multi_total = await redis_client.get("lb:multi_exec_total")
    lines.append(f"ai_lb_multi_exec_total {int(multi_total) if multi_total else 0}")
    for mode in ("race", "all", "sequence", "consensus"):
        val = await redis_client.get(f"lb:multi_exec_total:{mode}")
        if val:
            lines.append(f'ai_lb_multi_exec_total{{mode="{mode}"}} {int(val)}')

    lines.append("# HELP ai_lb_multi_exec_backends Backends attempted per multi-exec request by mode")
    lines.append("# TYPE ai_lb_multi_exec_backends summary")
    for mode in ("race", "all", "sequence", "consensus"):
        s_sum = await redis_client.get(f"lb:multi_exec_backends_sum:{mode}")
        s_cnt = await redis_client.get(f"lb:multi_exec_backends_count:{mode}")
        if s_cnt and int(s_cnt) > 0:
            lines.append(f'ai_lb_multi_exec_backends_sum{{mode="{mode}"}} {int(s_sum) if s_sum else 0}')
            lines.append(f'ai_lb_multi_exec_backends_count{{mode="{mode}"}} {int(s_cnt)}')

    lines.append("# HELP ai_lb_multi_exec_succeeded Backends that succeeded per multi-exec request by mode")
    lines.append("# TYPE ai_lb_multi_exec_succeeded counter")
    for mode in ("race", "all", "sequence", "consensus"):
        val = await redis_client.get(f"lb:multi_exec_succeeded_sum:{mode}")
        if val:
            lines.append(f'ai_lb_multi_exec_succeeded{{mode="{mode}"}} {int(val)}')

    # Consensus-specific metrics
    lines.append("# HELP ai_lb_consensus_total Total consensus requests")
    lines.append("# TYPE ai_lb_consensus_total counter")
    consensus_total = await redis_client.get("lb:consensus_total")
    lines.append(f"ai_lb_consensus_total {int(consensus_total) if consensus_total else 0}")

    # Per-model consensus totals
    consensus_models = await redis_client.smembers("lb:consensus_models")
    for model in consensus_models:
        val = await redis_client.get(f"lb:consensus_total:{model}")
        if val:
            lines.append(f'ai_lb_consensus_total{{model="{model}"}} {int(val)}')

    # Consensus agreements/disagreements
    lines.append("# HELP ai_lb_consensus_agreements Total consensus agreements (unanimous)")
    lines.append("# TYPE ai_lb_consensus_agreements counter")
    agreements = await redis_client.get("lb:consensus_agreements")
    lines.append(f"ai_lb_consensus_agreements {int(agreements) if agreements else 0}")

    lines.append("# HELP ai_lb_consensus_disagreements Total consensus disagreements (not unanimous)")
    lines.append("# TYPE ai_lb_consensus_disagreements counter")
    disagreements = await redis_client.get("lb:consensus_disagreements")
    lines.append(f"ai_lb_consensus_disagreements {int(disagreements) if disagreements else 0}")

    # Per-model agreements/disagreements
    for model in consensus_models:
        agree_val = await redis_client.get(f"lb:consensus_agreements:{model}")
        if agree_val:
            lines.append(f'ai_lb_consensus_agreements{{model="{model}"}} {int(agree_val)}')
        disagree_val = await redis_client.get(f"lb:consensus_disagreements:{model}")
        if disagree_val:
            lines.append(f'ai_lb_consensus_disagreements{{model="{model}"}} {int(disagree_val)}')

    # Agreement count distribution (histogram of how many backends agreed)
    lines.append("# HELP ai_lb_consensus_agreement_count Agreement count distribution")
    lines.append("# TYPE ai_lb_consensus_agreement_count counter")
    for count in (0, 1, 2, 3):
        val = await redis_client.get(f"lb:consensus_agreement:{count}")
        if val:
            lines.append(f'ai_lb_consensus_agreement_count{{count="{count}"}} {int(val)}')

    # Per-model agreement counts
    for model in consensus_models:
        for count in (0, 1, 2, 3):
            val = await redis_client.get(f"lb:consensus_agreement:{model}:{count}")
            if val:
                lines.append(f'ai_lb_consensus_agreement_count{{model="{model}",count="{count}"}} {int(val)}')

    # Comparison type distribution
    lines.append("# HELP ai_lb_consensus_comparison_type Comparison type used for consensus")
    lines.append("# TYPE ai_lb_consensus_comparison_type counter")
    for comp_type in ("hash", "text", "tool_calls", "single", "none"):
        val = await redis_client.get(f"lb:consensus_comparison:{comp_type}")
        if val:
            lines.append(f'ai_lb_consensus_comparison_type{{type="{comp_type}"}} {int(val)}')

    # Per-model comparison types
    for model in consensus_models:
        for comp_type in ("hash", "text", "tool_calls", "single", "none"):
            val = await redis_client.get(f"lb:consensus_comparison:{model}:{comp_type}")
            if val:
                lines.append(f'ai_lb_consensus_comparison_type{{model="{model}",type="{comp_type}"}} {int(val)}')

    content = "\n".join(lines) + "\n"
    return Response(content=content, media_type="text/plain; version=0.0.4; charset=utf-8")

@app.post("/v1/admin/prefs")
async def set_prefs(request: Request):
    """Hot update preferences: preferred models, weights, caps.
    Body keys (all optional):
      - preferred_models: [str]
      - model_weights: {model_id: float}
      - model_caps: {model_id: int}
      - node_caps: {"host:port": int}
      - auto_model_strategy: "any_first" | "intersection_first"
    """
    body = await request.json()
    applied = {}
    pm = body.get("preferred_models")
    if isinstance(pm, list):
        config.PREFERRED_MODELS[:] = [str(x) for x in pm]
        try:
            await redis_client.set("lb:prefs:preferred_models", json.dumps(config.PREFERRED_MODELS))
        except Exception:
            pass
        applied["preferred_models"] = config.PREFERRED_MODELS
    mw = body.get("model_weights")
    if isinstance(mw, dict):
        try:
            for k, v in mw.items():
                config.MODEL_WEIGHTS[str(k)] = float(v)
        except Exception:
            pass
        try:
            await redis_client.set("lb:model_weights", json.dumps({k: float(v) for k, v in mw.items()}))
        except Exception:
            pass
        applied["model_weights"] = {k: float(v) for k, v in mw.items()}
    mc = body.get("model_caps")
    if isinstance(mc, dict):
        for mid, cap in mc.items():
            try:
                await redis_client.set(f"model:{mid}:maxconn", int(cap))
            except Exception:
                continue
        applied["model_caps"] = {k: int(v) for k, v in mc.items()}
    nc = body.get("node_caps")
    if isinstance(nc, dict):
        for node, cap in nc.items():
            try:
                await redis_client.set(f"node:{node}:maxconn", int(cap))
            except Exception:
                continue
        applied["node_caps"] = {k: int(v) for k, v in nc.items()}
    strat = body.get("auto_model_strategy")
    if isinstance(strat, str) and strat.lower() in ("any_first", "intersection_first"):
        config.AUTO_MODEL_STRATEGY = strat
        applied["auto_model_strategy"] = strat
    return {"ok": True, "applied": applied}


@app.post("/v1/admin/reset_histograms")
async def reset_histograms(request: Request):
    """Admin: reset/delete histogram and series metrics for a given model.

    Body (JSON):
      - model: str (required)
      - nodes: [str] (optional). If omitted, autodetect nodes from existing series sets.
      - include: ["latency", "stream_ttfb", "stream_duration"] (optional). Default: all three.
      - dry_run: bool (optional, default false). If true, return keys that would be deleted without mutating.

    Returns JSON with counts and details.
    """
    body = await request.json()
    model = (body.get("model") or "").strip()
    if not model:
        raise HTTPException(status_code=400, detail="Field 'model' is required.")

    nodes = body.get("nodes")
    include = body.get("include") or ["latency", "stream_ttfb", "stream_duration"]
    dry_run = bool(body.get("dry_run", False))

    # Autodetect nodes from series sets if not provided
    if not nodes:
        nodes_detected = set()
        try:
            lat_series = await redis_client.smembers("lb:latency_series")
            for s in lat_series:
                try:
                    m, n = s.split("|", 1)
                    if m == model:
                        nodes_detected.add(n)
                except Exception:
                    continue
        except Exception:
            pass
        try:
            ttfb_series = await redis_client.smembers("lb:stream_ttfb_series")
            for s in ttfb_series:
                try:
                    m, n = s.split("|", 1)
                    if m == model:
                        nodes_detected.add(n)
                except Exception:
                    continue
        except Exception:
            pass
        try:
            dur_series = await redis_client.smembers("lb:stream_duration_series")
            for s in dur_series:
                try:
                    m, n = s.split("|", 1)
                    if m == model:
                        nodes_detected.add(n)
                except Exception:
                    continue
        except Exception:
            pass
        nodes = sorted(nodes_detected)

    # Build key lists to delete per node
    deleted = 0
    keys_preview = []
    series_removed = 0
    details = []

    async def _collect_keys(series_key: str) -> list[str]:
        out = []
        if "latency" in include:
            out.append(f"lb:latency_sum:{series_key}")
            out.append(f"lb:latency_count:{series_key}")
            try:
                bucket_keys = await redis_client.keys(f"lb:latency_bucket:{series_key}:*")
                out.extend(bucket_keys or [])
            except Exception:
                pass
        if "stream_ttfb" in include:
            out.append(f"lb:stream_ttfb_sum:{series_key}")
            out.append(f"lb:stream_ttfb_count:{series_key}")
            try:
                ttfb_keys = await redis_client.keys(f"lb:stream_ttfb_bucket:{series_key}:*")
                out.extend(ttfb_keys or [])
            except Exception:
                pass
        if "stream_duration" in include:
            out.append(f"lb:stream_duration_sum:{series_key}")
            out.append(f"lb:stream_duration_count:{series_key}")
            try:
                dur_keys = await redis_client.keys(f"lb:stream_duration_bucket:{series_key}:*")
                out.extend(dur_keys or [])
            except Exception:
                pass
        return out

    for n in nodes or []:
        s = f"{model}|{n}"
        keys = await _collect_keys(s)
        keys_preview.extend(keys)
        if not dry_run:
            try:
                if "latency" in include:
                    try:
                        removed = await redis_client.srem("lb:latency_series", s)
                        series_removed += int(removed or 0)
                    except Exception:
                        pass
                if "stream_ttfb" in include:
                    try:
                        removed = await redis_client.srem("lb:stream_ttfb_series", s)
                        series_removed += int(removed or 0)
                    except Exception:
                        pass
                if "stream_duration" in include:
                    try:
                        removed = await redis_client.srem("lb:stream_duration_series", s)
                        series_removed += int(removed or 0)
                    except Exception:
                        pass
                # Delete keys in batches
                for i in range(0, len(keys), 256):
                    chunk = keys[i:i+256]
                    if chunk:
                        deleted += await redis_client.delete(*chunk)
                details.append({"node": n, "series": s, "keys_deleted": len(keys)})
            except Exception as e:
                details.append({"node": n, "series": s, "error": str(e)})
        else:
            details.append({"node": n, "series": s, "keys_preview": len(keys)})

    return {
        "ok": True,
        "model": model,
        "nodes": nodes or [],
        "include": include,
        "dry_run": dry_run,
        "keys_preview": len(keys_preview) if dry_run else None,
        "keys_deleted": deleted if not dry_run else None,
        "series_removed": series_removed if not dry_run else None,
        "details": details,
    }
